{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "FinalPhase_NMT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUf1lKg4FwpL"
      },
      "source": [
        "\n",
        "\n",
        "## **Seq2Seq Model with Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srr0PfgVxSXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7110ab73-b194-464b-99f2-d5876a72a6b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gToUbjaidr7F"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "import string\n",
        "import operator\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K_1HAD21ZCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89764f43-c081-4d90-90d3-3d7734bbdd21"
      },
      "source": [
        "# Install Indic NLP Library\n",
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "from indicnlp import loader\n",
        "loader.load()\n",
        "\n",
        "from indicnlp.tokenize import indic_detokenize, indic_tokenize"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1325, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 1325 (delta 84), reused 89 (delta 41), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1325/1325), 9.57 MiB | 11.21 MiB/s, done.\n",
            "Resolving deltas: 100% (688/688), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (133/133), 149.77 MiB | 32.96 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Collecting Morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DgXNXiJ2rfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32f4d2d-fe2c-4b16-bc63-002fbc824c1c"
      },
      "source": [
        "# Install NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmF8B1MqSi8j"
      },
      "source": [
        "### **Read the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iVFS7n30Si8k"
      },
      "source": [
        "def read_dataset(filepath):\n",
        "\n",
        "  hindi_list=[]\n",
        "  english_list=[]\n",
        "\n",
        "  with open(filepath, 'r') as f:\n",
        "      reader = csv.DictReader(f)\n",
        "      for line in reader:\n",
        "          hindi_list.append(line['hindi'])\n",
        "          english_list.append(line['english'])\n",
        "\n",
        "  return hindi_list, english_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh_hDj_GSi8l"
      },
      "source": [
        "\n",
        "\n",
        "### **Train and Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "P2wZW6eASi8m"
      },
      "source": [
        "def train_test_split(hindi_list, english_list, train_size=0.8):\n",
        "\n",
        "  indices_list = np.arange(len(hindi_list))\n",
        "  np.random.shuffle(indices_list)\n",
        "  \n",
        "  train_indices_list = indices_list[:int(len(indices_list)*train_size)]\n",
        "  test_indices_list = indices_list[int(len(indices_list)*train_size):]\n",
        "\n",
        "  hindi_sentence_list = [hindi_list[i] for i in train_indices_list]\n",
        "  english_sentence_list = [english_list[i] for i in train_indices_list]\n",
        "\n",
        "  hindi_test_sentence_list = [hindi_list[i] for i in test_indices_list]\n",
        "  english_test_sentence_list = [english_list[i] for i in test_indices_list]\n",
        "\n",
        "  return hindi_sentence_list, english_sentence_list, hindi_test_sentence_list, english_test_sentence_list\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYl4kOy50YJa"
      },
      "source": [
        "# Read from stored files to load the model\n",
        "def load_train_test():\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/FinalRound7/hindi_train.txt','r') as f:\n",
        "    hindi_sentence_list = f.read().splitlines()\n",
        "\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/FinalRound7/english_train.txt','r') as f:\n",
        "    english_sentence_list = f.read().splitlines()\n",
        "\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/FinalRound7/english_test.txt','r') as f:\n",
        "    english_test_sentence_list = f.read().splitlines()\n",
        "\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/FinalRound7/hindi_test.txt','r') as f:\n",
        "    hindi_test_sentence_list = f.read().splitlines()\n",
        "\n",
        "  return hindi_sentence_list, english_sentence_list, hindi_test_sentence_list, english_test_sentence_list"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOKGcupGGPvD"
      },
      "source": [
        "### **Tokenize Hindi sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CFni1UAG9y9",
        "trusted": true
      },
      "source": [
        "# Detokenize\n",
        "# Ref: https://colab.research.google.com/drive/1p3oGPcNdORw5_MDcufTDYWJhJt3XVPuC?usp=sharing#scrollTo=GU6E07Yw5zvl\n",
        "\n",
        "def detokenize(sent_list, lang='hi'):  # pass hindi_sentence_list and lang='hi'\n",
        "\n",
        "  for i in range(len(sent_list)):\n",
        "    sent_list[i] = indic_detokenize.trivial_detokenize(sent_list[i],lang)\n",
        "\n",
        "  return sent_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytzhhOAqyY-R",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0cc0afcb-cb0a-44cb-8c40-96abf179c91f"
      },
      "source": [
        " string.punctuation"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQdSsjw8Nqwz",
        "trusted": true
      },
      "source": [
        "#Ref for unicode chart: https://www.ssec.wisc.edu/~tomw/java/unicode.html#x0900 \n",
        "\n",
        "def hindi_tokenize(hindi_sentence_list): \n",
        "\n",
        "  hindi_word_to_count={}\n",
        "  hindi_word_to_index={'UNK':0, 'PAD':1, 'SOS':2, 'EOS':3}\n",
        "  hindi_index_to_word={0:'UNK', 1:'PAD', 2:'SOS', 3:'EOS'}\n",
        "  count=4\n",
        "  for sent in hindi_sentence_list:\n",
        "    for t in indic_tokenize.trivial_tokenize(sent): \n",
        "      x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "      for elem in x:\n",
        "        hindi_word_to_count[elem] = hindi_word_to_count.get(elem,0)+1\n",
        "        if hindi_word_to_index.get(elem) is None and hindi_word_to_count.get(elem,0) >= 2:\n",
        "          hindi_word_to_index[elem] = count\n",
        "          hindi_index_to_word[count] = elem\n",
        "          count+=1\n",
        "  return hindi_word_to_index, hindi_index_to_word"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSfh63mXqitj"
      },
      "source": [
        "### **Tokenize English sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCGsCkw01Po3",
        "trusted": true
      },
      "source": [
        "def english_tokenize(english_sentence_list):\n",
        "\n",
        "  english_word_to_count={}\n",
        "  english_word_to_index={'UNK':0, 'PAD':1, 'SOS':2, 'EOS':3}\n",
        "  english_index_to_word={0:'UNK', 1:'PAD', 2:'SOS', 3:'EOS'}\n",
        "  count=4\n",
        "\n",
        "  for sent in english_sentence_list:\n",
        "    for token in word_tokenize(sent.lower()):\n",
        "      temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token)\n",
        "      for elem in temp:\n",
        "        english_word_to_count[elem] = english_word_to_count.get(elem,0)+1\n",
        "        if english_word_to_index.get(elem) is None and english_word_to_count.get(elem,0) >= 2:\n",
        "          english_word_to_index[elem] = count\n",
        "          english_index_to_word[count] = elem\n",
        "          count+=1\n",
        "  return english_word_to_index, english_index_to_word"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFcL5zxv9J62"
      },
      "source": [
        "### **Store the train and test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYlAUbE2lxi9"
      },
      "source": [
        "def store_lists(hindi_sentence_list, english_sentence_list, hindi_test_sentence_list, english_test_sentence_list):\n",
        "\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/FinalRound7/hindi_train.txt','w') as f:\n",
        "    for sent in hindi_sentence_list:\n",
        "      f.write(sent+'\\n')\n",
        "\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/FinalRound7/english_train.txt','w') as f:\n",
        "    for sent in english_sentence_list:\n",
        "      f.write(sent+'\\n')\n",
        "  \n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/FinalRound7/hindi_test.txt','w') as f:\n",
        "    for sent in hindi_test_sentence_list:\n",
        "      f.write(sent+'\\n')\n",
        "\n",
        "  with open('/content/drive/MyDrive/Colab Notebooks/FinalRound7/english_test.txt','w') as f:\n",
        "    for sent in english_test_sentence_list:\n",
        "      f.write(sent+'\\n')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJvOrOpvefHa"
      },
      "source": [
        "### **Find Sentences Length and fix the maximum length**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s49M8J3ed4R",
        "trusted": true
      },
      "source": [
        "def get_max_len(hindi_sentence_list, english_sentence_list, min_frequency=1000):\n",
        "\n",
        "  # check for hindi sentences\n",
        "  sent_len_count={}\n",
        "  for sent in hindi_sentence_list:\n",
        "    sent_len=0\n",
        "    for t in indic_tokenize.trivial_tokenize(sent): \n",
        "      x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "      for elem in x:\n",
        "        sent_len+=1\n",
        "    sent_len_count[sent_len] = sent_len_count.get(sent_len,0)+1\n",
        "\n",
        "    # sort the dictionary based on their counts\n",
        "  sorted_counts = sorted(sent_len_count.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "  index=0\n",
        "  for pair in sorted_counts:\n",
        "    if pair[1]>min_frequency:\n",
        "      index+=1\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  max_hindi_len=0\n",
        "  for pair in sorted_counts[:index]:\n",
        "    if pair[0]>max_hindi_len:\n",
        "      max_hindi_len=pair[0]\n",
        "\n",
        "\n",
        "  # Now check for english sentences\n",
        "  english_sent_len_count={}\n",
        "  for sent in english_sentence_list:\n",
        "    sent_len=0\n",
        "    for token in word_tokenize(sent.lower()): \n",
        "      temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token)\n",
        "      for elem in temp:\n",
        "        sent_len+=1\n",
        "    english_sent_len_count[sent_len] = english_sent_len_count.get(sent_len,0)+1\n",
        "\n",
        "  # sort the dictionary based on their counts\n",
        "  english_sorted_counts = sorted(english_sent_len_count.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "  index=0\n",
        "  for pair in english_sorted_counts:\n",
        "    if pair[1]>min_frequency:\n",
        "      index+=1\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  max_english_len=0\n",
        "  for pair in english_sorted_counts[:index]:\n",
        "    if pair[0]>max_english_len:\n",
        "      max_english_len=pair[0]\n",
        "\n",
        "  max_len = max(max_hindi_len, max_english_len)\n",
        "  \n",
        "  return max_len\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAbUhu62u18-"
      },
      "source": [
        "### **Filter out sentences with length less than or equal to maximum length**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP5Vr6pfu1AV",
        "trusted": true
      },
      "source": [
        "def filter_sentences(hindi_sentence_list, english_sentence_list, max_len):\n",
        "\n",
        "  english_filtered_sent_list=[]\n",
        "  hindi_filtered_sent_list=[]\n",
        "  filtered_sent_pair_list=[]\n",
        "  for hin_sent, eng_sent in zip(hindi_sentence_list, english_sentence_list):\n",
        "    hin_sent_len=0\n",
        "    for t in indic_tokenize.trivial_tokenize(hin_sent): \n",
        "      x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "      for elem in x:\n",
        "        hin_sent_len+=1\n",
        "\n",
        "    eng_sent_len=0\n",
        "    for token in word_tokenize(eng_sent.lower()): \n",
        "      temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token)\n",
        "      for elem in temp:\n",
        "        eng_sent_len+=1\n",
        "\n",
        "    if hin_sent_len>=1 and hin_sent_len<=max_len and eng_sent_len>=1 and eng_sent_len<=max_len:\n",
        "      english_filtered_sent_list.append(eng_sent)\n",
        "      hindi_filtered_sent_list.append(hin_sent)\n",
        "      filtered_sent_pair_list.append([hin_sent, eng_sent])\n",
        "\n",
        "  return english_filtered_sent_list, hindi_filtered_sent_list"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DciCZiuq3P8O"
      },
      "source": [
        "### **Form List of list of indexes from the filtered sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56Dc9KvM3NOC",
        "trusted": true
      },
      "source": [
        "\n",
        "\n",
        "def make_tensors( hindi_filtered_sent_list, hindi_test_sentence_list, english_filtered_sent_list,  english_test_sentence_list):\n",
        "  # Create tensor array for each hindi sentence \n",
        "  hindi_list_indices = torch.tensor([[1]*(max_len+2)]*len(hindi_filtered_sent_list), dtype=torch.long)\n",
        "\n",
        "  i=0\n",
        "  for sent in hindi_filtered_sent_list:\n",
        "    hindi_list_indices[i][0]=2  #SOS\n",
        "    j=1\n",
        "    for t in indic_tokenize.trivial_tokenize(sent): \n",
        "      x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "      for elem in x:\n",
        "        if hindi_word_to_index.get(elem) is None:\n",
        "          #continue\n",
        "          hindi_list_indices[i][j] = 0  # UNK\n",
        "        else:\n",
        "          hindi_list_indices[i][j] = hindi_word_to_index.get(elem)\n",
        "        j+=1\n",
        "    hindi_list_indices[i][j]=3  #EOS\n",
        "    j+=1\n",
        "    while j<=(max_len+1):\n",
        "      hindi_list_indices[i][j] = 1  #PAD\n",
        "      j+=1\n",
        "\n",
        "    i+=1\n",
        "\n",
        "  # Create tensor array for each hindi test sentence\n",
        "  hindi_test_list_indices = torch.tensor([[1]*(max_len+2)]*len(hindi_test_sentence_list), dtype=torch.long)\n",
        "\n",
        "  i=0\n",
        "  for sent in hindi_test_sentence_list:\n",
        "    hindi_test_list_indices[i][0]=2  #SOS\n",
        "    j=1\n",
        "    for t in indic_tokenize.trivial_tokenize(sent): \n",
        "      x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "      for elem in x:\n",
        "        if j>(max_len):\n",
        "          break\n",
        "        if hindi_word_to_index.get(elem) is None:\n",
        "          #continue\n",
        "          hindi_test_list_indices[i][j] = 0  # UNK\n",
        "        else:\n",
        "          hindi_test_list_indices[i][j] = hindi_word_to_index.get(elem)\n",
        "        j+=1\n",
        "    hindi_test_list_indices[i][j]=3  #EOS\n",
        "    j+=1\n",
        "    while j<=(max_len+1):\n",
        "      hindi_test_list_indices[i][j] = 1  #PAD\n",
        "      j+=1\n",
        "\n",
        "    i+=1\n",
        "\n",
        "  # Create tensor array for each english sentence\n",
        "\n",
        "  english_list_indices = torch.tensor([[1]*(max_len+2)]*len(english_filtered_sent_list), dtype=torch.long)\n",
        "\n",
        "  i=0\n",
        "  for sent in english_filtered_sent_list:\n",
        "    english_list_indices[i][0]=2  #SOS\n",
        "    j=1\n",
        "    for token in word_tokenize(sent.lower()):\n",
        "      temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token)\n",
        "      for elem in temp:\n",
        "        if english_word_to_index.get(elem) is None:\n",
        "          #continue\n",
        "          english_list_indices[i][j] = 0  # UNK\n",
        "        else:\n",
        "          english_list_indices[i][j] = english_word_to_index.get(elem)\n",
        "        j+=1\n",
        "    english_list_indices[i][j]=3  #EOS\n",
        "    j+=1\n",
        "    while j<=(max_len+1):\n",
        "      english_list_indices[i][j] = 1  #PAD\n",
        "      j+=1\n",
        "    \n",
        "    i+=1\n",
        "\n",
        "  # Create tensor array for each english test sentence\n",
        "  english_test_list_indices = torch.tensor([[1]*(max_len+2)]*len(english_test_sentence_list), dtype=torch.long)\n",
        "\n",
        "  i=0\n",
        "  for sent in english_test_sentence_list:\n",
        "    english_test_list_indices[i][0]=2  #SOS\n",
        "    j=1\n",
        "    for token in word_tokenize(sent.lower()):\n",
        "      temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token)\n",
        "      for elem in temp:\n",
        "        if j>(max_len):\n",
        "          break\n",
        "        if english_word_to_index.get(elem) is None:\n",
        "          #continue\n",
        "          english_test_list_indices[i][j] = 0  # UNK\n",
        "        else:\n",
        "          english_test_list_indices[i][j] = english_word_to_index.get(elem)\n",
        "        j+=1\n",
        "    english_test_list_indices[i][j]=3  #EOS\n",
        "    j+=1\n",
        "    while j<=(max_len+1):\n",
        "      english_test_list_indices[i][j] = 1  #PAD\n",
        "      j+=1\n",
        "    \n",
        "    i+=1\n",
        "\n",
        "  return hindi_list_indices, hindi_test_list_indices, english_list_indices, english_test_list_indices"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gz9IeEYSi83"
      },
      "source": [
        "#### **Use GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RM5t4w5-kkF",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756dc6d4-b9a3-44f4-beb5-ca7e87a20c0d"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "print(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9381KgYUBuu"
      },
      "source": [
        "### **References:** \n",
        "### http://ethen8181.github.io/machine-learning/deep_learning/seq2seq/2_torch_seq2seq_attention.html \n",
        "### https://arxiv.org/abs/1409.0473"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppc7x4YCSi84"
      },
      "source": [
        "## **Sequence to Sequence Model with Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiDTgw5014t_",
        "trusted": true
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, embedding_size, hidden_dimension, num_layers, dropout):  \n",
        "    super().__init__()\n",
        "    \n",
        "    self.input_size=input_size\n",
        "    self.dropout=dropout\n",
        "    self.hidden_dimension = hidden_dimension\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "    self.rnn = nn.GRU(embedding_size, hidden_dimension,num_layers, dropout=dropout, bidirectional=True)\n",
        "\n",
        "    self.final_layer = nn.Linear(hidden_dimension * 2, hidden_dimension)\n",
        "\n",
        "  def forward(self, word_inputs):\n",
        " \n",
        "    embedded = self.embedding(word_inputs)\n",
        "\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "    #outputs shape = [sequence_length, batch_size, hidden_dimension*num_of_directions]\n",
        "\n",
        "    x = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "    hidden = torch.tanh(self.final_layer(x))\n",
        "\n",
        "    return outputs, hidden"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-kgdIYkJo3G",
        "trusted": true
      },
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dimension):\n",
        "        super().__init__()\n",
        "        self.hidden_dimension = hidden_dimension\n",
        "\n",
        "        self.final_layer1 = nn.Linear(hidden_dimension * 2 + hidden_dimension, hidden_dimension)\n",
        "        self.final_layer2 = nn.Linear(hidden_dimension, 1, bias=False)\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden):\n",
        "        sequence_length = encoder_outputs.shape[0]\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        \n",
        "        hidden = hidden.unsqueeze(1).repeat(1, sequence_length, 1)\n",
        "\n",
        "        outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        x = torch.cat((hidden, outputs), dim=2)\n",
        "\n",
        "        energy = torch.tanh(self.final_layer1(x))\n",
        "\n",
        "        attention = self.final_layer2(energy).squeeze(dim=2)        \n",
        "        attention_weight = torch.softmax(attention, dim=1)\n",
        "\n",
        "        return attention_weight"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdKil5S03aZa",
        "trusted": true
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_size, embedding_size, hidden_dimension, num_layers, dropout, attention):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.embedding_size=embedding_size\n",
        "    self.output_size = output_size\n",
        "    self.hidden_dimension = hidden_dimension\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = dropout\n",
        "    self.attention = attention\n",
        "\n",
        "    self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "    \n",
        "    self.rnn = nn.GRU(hidden_dimension * 2 + embedding_size, hidden_dimension, num_layers, dropout = dropout)\n",
        "    \n",
        "    self.linear = nn.Linear(hidden_dimension, output_size)\n",
        "    \n",
        "  def forward(self, input, encoder_states, hidden):\n",
        "\n",
        "    attention = self.attention(encoder_states, hidden).unsqueeze(1)\n",
        "\n",
        "    outputs = encoder_states.permute(1, 0, 2)\n",
        "\n",
        "    context = torch.bmm(attention, outputs).permute(1, 0, 2)\n",
        "\n",
        "    embedded = self.embedding(input.unsqueeze(0))\n",
        "    x = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "    outputs, hidden = self.rnn(x, hidden.unsqueeze(0))\n",
        "    prediction = self.linear(outputs.squeeze(0))\n",
        "    return prediction, hidden.squeeze(0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaGtBf8Xv0Xz",
        "trusted": true
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "      \n",
        "  def forward(self, source, target, teacher_force = 1):\n",
        "\n",
        "    batch_size = target.shape[1]\n",
        "    sequence_length = target.shape[0]\n",
        "\n",
        "    target_dict_size = self.decoder.output_size\n",
        "    \n",
        "    pred_output = torch.zeros(sequence_length, batch_size, target_dict_size).to(self.device)\n",
        "    \n",
        "    encoder_states, hidden = self.encoder(source)\n",
        "    \n",
        "    input = target[0]\n",
        "    \n",
        "    for i in range(1, sequence_length):\n",
        "\n",
        "      output, hidden = self.decoder(input, encoder_states, hidden)\n",
        "\n",
        "      pred_output[i] = output\n",
        "    \n",
        "      best_pred = output.argmax(1) \n",
        "\n",
        "      if random.random() < teacher_force:\n",
        "        input = target[i]\n",
        "\n",
        "      else:\n",
        "        input = best_pred\n",
        "    \n",
        "    return pred_output\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eQNzannHm_T"
      },
      "source": [
        "### **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAc_R66GHmIJ"
      },
      "source": [
        "filepath = '/content/drive/MyDrive/Colab Notebooks/train.csv'\n",
        "\n",
        "# read dataset\n",
        "hindi_list, english_list = read_dataset(filepath)\n",
        "\n",
        "# split into train and test data\n",
        "hindi_sentence_list, english_sentence_list, hindi_test_sentence_list, english_test_sentence_list = train_test_split(hindi_list, english_list, train_size=1)\n",
        "\n",
        "# Detokenize hindi sentences\n",
        "hindi_sentence_list = detokenize(hindi_sentence_list, lang='hi')\n",
        "\n",
        "# Store the train and test sentences\n",
        "store_lists(hindi_sentence_list, english_sentence_list, hindi_test_sentence_list, english_test_sentence_list)\n",
        "\n",
        "# Tokenize hindi sentences\n",
        "hindi_word_to_index, hindi_index_to_word = hindi_tokenize(hindi_sentence_list)\n",
        "\n",
        "# Tokenize english sentences\n",
        "english_word_to_index, english_index_to_word = english_tokenize(english_sentence_list)\n",
        "\n",
        "# Set maximum length\n",
        "max_len = get_max_len(hindi_sentence_list, english_sentence_list, min_frequency=300)\n",
        "\n",
        "# Filter the sentences greater than max length\n",
        "english_filtered_sent_list, hindi_filtered_sent_list = filter_sentences(hindi_sentence_list, english_sentence_list, max_len)\n",
        "\n",
        "# Form tensor of the sentences\n",
        "hindi_list_indices, hindi_test_list_indices, english_list_indices, english_test_list_indices = make_tensors(hindi_filtered_sent_list, hindi_test_sentence_list, english_filtered_sent_list,  english_test_sentence_list)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvECVNV77wFQ"
      },
      "source": [
        "### **Set Model Parameters and define Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqoT3XxmwqZu",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7dec3e0-dd0e-40bd-a0d8-d0511563b031"
      },
      "source": [
        "input_size = len(hindi_word_to_index)\n",
        "output_size = len(english_word_to_index)\n",
        "embedding_size = 128\n",
        "hidden_dimension = 512\n",
        "num_layers = 1\n",
        "\n",
        "dropout = 0.5\n",
        "batch_size = 32\n",
        "\n",
        "attention = Attention(hidden_dimension)\n",
        "enc = Encoder(input_size, embedding_size, hidden_dimension, num_layers, dropout).to(device)\n",
        "dec = Decoder(output_size, embedding_size, hidden_dimension, num_layers, dropout, attention).to(device)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "print(enc)\n",
        "print(dec)\n",
        "print(model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Encoder(\n",
            "  (embedding): Embedding(21672, 128)\n",
            "  (rnn): GRU(128, 512, dropout=0.5, bidirectional=True)\n",
            "  (final_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
            ")\n",
            "Decoder(\n",
            "  (attention): Attention(\n",
            "    (final_layer1): Linear(in_features=1536, out_features=512, bias=True)\n",
            "    (final_layer2): Linear(in_features=512, out_features=1, bias=False)\n",
            "  )\n",
            "  (embedding): Embedding(19009, 128)\n",
            "  (rnn): GRU(1152, 512, dropout=0.5)\n",
            "  (linear): Linear(in_features=512, out_features=19009, bias=True)\n",
            ")\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(21672, 128)\n",
            "    (rnn): GRU(128, 512, dropout=0.5, bidirectional=True)\n",
            "    (final_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (attention): Attention(\n",
            "      (final_layer1): Linear(in_features=1536, out_features=512, bias=True)\n",
            "      (final_layer2): Linear(in_features=512, out_features=1, bias=False)\n",
            "    )\n",
            "    (embedding): Embedding(19009, 128)\n",
            "    (rnn): GRU(1152, 512, dropout=0.5)\n",
            "    (linear): Linear(in_features=512, out_features=19009, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMaINRf5cpi"
      },
      "source": [
        "### **Test Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXy9rWYl7ugV",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f0d6bc-57cd-4e2e-ba10-da3aba61900c"
      },
      "source": [
        "word_input = torch.zeros((7, 4), dtype=torch.long, device=device)  # here 7 is seq length and 4 is batch size\n",
        "print(word_input.shape)\n",
        "out, enc_hid = enc(word_input)  # encode this word_input\n",
        "\n",
        "print(enc_hid)\n",
        "\n",
        "print(enc_hid.shape) # [num_layers, seq_length, hidden_units]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([7, 4])\n",
            "tensor([[-0.2043, -0.0622,  0.0095,  ..., -0.2569, -0.1148, -0.3100],\n",
            "        [-0.2043, -0.0622,  0.0095,  ..., -0.2569, -0.1148, -0.3100],\n",
            "        [-0.2043, -0.0622,  0.0095,  ..., -0.2569, -0.1148, -0.3100],\n",
            "        [-0.2043, -0.0622,  0.0095,  ..., -0.2569, -0.1148, -0.3100]],\n",
            "       device='cuda:0', grad_fn=<TanhBackward>)\n",
            "torch.Size([4, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2eHI-7VB1tY"
      },
      "source": [
        "### **Test Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5RUUFp-_9WA",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa5b313-844d-448b-d819-b8cb16f6cc00"
      },
      "source": [
        "for i in range(7):\n",
        "    input = word_input[i]\n",
        "    pred, dec_hid= dec(input, out, enc_hid)\n",
        "    print(dec_hid.shape, pred)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 512]) tensor([[-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "torch.Size([4, 512]) tensor([[-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "torch.Size([4, 512]) tensor([[-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "torch.Size([4, 512]) tensor([[-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "torch.Size([4, 512]) tensor([[-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "torch.Size([4, 512]) tensor([[-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "torch.Size([4, 512]) tensor([[-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978],\n",
            "        [-0.0881, -0.1821, -0.1778,  ..., -0.0157, -0.0460,  0.1978]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oLUvmCQPsGD"
      },
      "source": [
        "### **Initialize Weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjLV2gluyIcI",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7158ebc3-85c4-4fec-9c0f-2249bd1fade5"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(21672, 128)\n",
              "    (rnn): GRU(128, 512, dropout=0.5, bidirectional=True)\n",
              "    (final_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (final_layer1): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (final_layer2): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(19009, 128)\n",
              "    (rnn): GRU(1152, 512, dropout=0.5)\n",
              "    (linear): Linear(in_features=512, out_features=19009, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ieuR_dK0Jwn"
      },
      "source": [
        "### **Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Htf9iuFfAcv",
        "trusted": true
      },
      "source": [
        "from torch import optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJEV_8v24MZY"
      },
      "source": [
        "### **Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MepBgJv64Lrt",
        "trusted": true
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = 1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv9QJw6HSi89"
      },
      "source": [
        "### **Create Batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H9nw2k6po9a",
        "trusted": true
      },
      "source": [
        "from torch.utils import data\n",
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "    dataset = data.TensorDataset(*data_arrays)\n",
        "    return data.DataLoader(dataset, batch_size, shuffle=is_train)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYB9thb3zO51",
        "trusted": true
      },
      "source": [
        "data_arrays = (hindi_list_indices, english_list_indices)\n",
        "data_iter = load_array(data_arrays, batch_size)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAVsOrJGSi8-"
      },
      "source": [
        "### **Translate Hindi Sentence to English Sentence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvXjTt4XcXqv",
        "trusted": true
      },
      "source": [
        "def translate_sentence(model, sentence, device, max_length=max_len):\n",
        "\n",
        "    sent_list=[]\n",
        "    for t in indic_tokenize.trivial_tokenize(sentence): \n",
        "      x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "      for elem in x:\n",
        "        if hindi_word_to_index.get(elem) is None:\n",
        "          sent_list.append(hindi_word_to_index['UNK'])\n",
        "          #continue\n",
        "        else:\n",
        "           sent_list.append(hindi_word_to_index[elem]) \n",
        "\n",
        "    sent_list.insert(0, hindi_word_to_index['SOS'])\n",
        "    sent_list.append(hindi_word_to_index['EOS'])\n",
        "\n",
        "    while(len(sent_list)<max_len):\n",
        "      sent_list.append(hindi_word_to_index['PAD'])\n",
        "\n",
        "\n",
        "    sent_tensor = torch.tensor(sent_list, dtype=torch.long).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs_encoder, hidden = model.encoder(sent_tensor)\n",
        "\n",
        "    outputs = [2]  #SOS = 2\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(previous_word, outputs_encoder, hidden)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        #EOS=3\n",
        "        if output.argmax(1).item() == 3:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english_index_to_word[idx] for idx in outputs]\n",
        "\n",
        "    return translated_sentence[1:]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m_h_VVUSi8-"
      },
      "source": [
        "### **Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oKYdrzUfSi8-"
      },
      "source": [
        "def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n",
        "    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n",
        "    torch.save(state, '/content/drive/MyDrive/Colab Notebooks/FinalRound7/checkpoint-NMT')\n",
        "    torch.save(model.state_dict(),'/content/drive/MyDrive/Colab Notebooks/FinalRound7/checkpoint-NMT-SD')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg91N77KSi8_"
      },
      "source": [
        "### **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTfYPg54bIL1",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50bd6d86-5ffb-409e-e605-d298666e2a23"
      },
      "source": [
        "import random\n",
        "import sys\n",
        "epoch_loss = 0.0\n",
        "num_epochs = 15\n",
        "best_loss = sys.maxsize\n",
        "best_epoch = -1\n",
        "step=0\n",
        "i=0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  print(\"Epoch -\",epoch+1)\n",
        "  model.eval()\n",
        "\n",
        "  model.train(True)\n",
        "  for batch_idx, batch in enumerate(data_iter):\n",
        "    input, target = [x.to(device) for x in batch]\n",
        "\n",
        "    input = input.permute(1,0)\n",
        "    target = target.permute(1,0)\n",
        "\n",
        "\n",
        "    output = model(input, target)\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "      \n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  if epoch_loss < best_loss:\n",
        "    best_loss = epoch_loss\n",
        "    best_epoch = epoch\n",
        "    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
        "\n",
        "  print(\"Epoch_Loss - {}\".format(loss.item()))\n",
        "  print()\n",
        "  \n",
        "print(epoch_loss / len(data_iter))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch - 1\n",
            "Epoch_Loss - 3.2839982509613037\n",
            "\n",
            "Epoch - 2\n",
            "Epoch_Loss - 2.755028247833252\n",
            "\n",
            "Epoch - 3\n",
            "Epoch_Loss - 1.7104363441467285\n",
            "\n",
            "Epoch - 4\n",
            "Epoch_Loss - 1.9817330837249756\n",
            "\n",
            "Epoch - 5\n",
            "Epoch_Loss - 1.3006936311721802\n",
            "\n",
            "Epoch - 6\n",
            "Epoch_Loss - 1.4060721397399902\n",
            "\n",
            "Epoch - 7\n",
            "Epoch_Loss - 0.9448722004890442\n",
            "\n",
            "Epoch - 8\n",
            "Epoch_Loss - 1.3367235660552979\n",
            "\n",
            "Epoch - 9\n",
            "Epoch_Loss - 0.792805016040802\n",
            "\n",
            "Epoch - 10\n",
            "Epoch_Loss - 0.8152564764022827\n",
            "\n",
            "Epoch - 11\n",
            "Epoch_Loss - 0.624953031539917\n",
            "\n",
            "Epoch - 12\n",
            "Epoch_Loss - 0.5024504661560059\n",
            "\n",
            "Epoch - 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-jQYNTSi8_"
      },
      "source": [
        "### **Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rT-vBCp-Si8_"
      },
      "source": [
        "# # Load the model\n",
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/FinalRound6/checkpoint-NMT-SD'))\n",
        "# hindi_sentence_list, english_sentence_list, hindi_test_sentence_list, english_test_sentence_list = load_train_test()\n",
        "\n",
        "# # Detokenize hindi sentences\n",
        "# hindi_sentence_list = detokenize(hindi_sentence_list, lang='hi')\n",
        "\n",
        "# # Store the train and test sentences\n",
        "# store_lists(hindi_sentence_list, english_sentence_list, hindi_test_sentence_list, english_test_sentence_list)\n",
        "\n",
        "# # Tokenize hindi sentences\n",
        "# hindi_word_to_index, hindi_index_to_word = hindi_tokenize(hindi_sentence_list)\n",
        "\n",
        "# # Tokenize english sentences\n",
        "# english_word_to_index, english_index_to_word = english_tokenize(english_sentence_list)\n",
        "\n",
        "# # Set maximum length\n",
        "# max_len = get_max_len(hindi_sentence_list, english_sentence_list, min_frequency=300)\n",
        "\n",
        "# # Filter the sentences greater than max length\n",
        "# english_filtered_sent_list, hindi_filtered_sent_list = filter_sentences(hindi_sentence_list, english_sentence_list, max_len)\n",
        "\n",
        "# # Form tensor of the sentences\n",
        "# hindi_list_indices, hindi_test_list_indices, english_list_indices, english_test_list_indices = make_tensors(hindi_filtered_sent_list, hindi_test_sentence_list, english_filtered_sent_list,  english_test_sentence_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE2mWj0rSi9A"
      },
      "source": [
        "### **Translate the Hindi Sentences from the Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2huZfH1B6Gx",
        "trusted": true
      },
      "source": [
        "outputs = []\n",
        "\n",
        "for src,trg in zip(hindi_test_sentence_list, english_test_sentence_list):\n",
        "    prediction = translate_sentence(model, src, device)\n",
        "    prediction = prediction[:-1]  # remove <eos> token\n",
        "    x = ' '.join([e for e in prediction])\n",
        "    outputs.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMcXRV_jSi9A"
      },
      "source": [
        "### **Compute BLEU and METEOR score on Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwnGPqaREaa1",
        "trusted": true
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C32flvd6BuBK",
        "trusted": true
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "nltk.download('wordnet')\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "\n",
        "references = english_test_sentence_list\n",
        "\n",
        "hypotheses = outputs\n",
        "\n",
        "total_num = len(references)\n",
        "total_bleu_scores = 0\n",
        "total_meteor_scores = 0\n",
        "for i in range(total_num):\n",
        "  total_bleu_scores+=sentence_bleu([references[i].split(\" \")], hypotheses[i].split(\" \"))\n",
        "  total_meteor_scores+=single_meteor_score(references[i], hypotheses[i])\n",
        "\n",
        "bleu_result = total_bleu_scores/total_num\n",
        "meteor_result = total_meteor_scores/total_num\n",
        "\n",
        "print(\"bleu score: \",bleu_result)\n",
        "print(\"meteor score: \",meteor_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOpKp2GuSi9B"
      },
      "source": [
        "### **Load week3 Hindi dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMmj7YiMul8b",
        "trusted": true
      },
      "source": [
        "testpath = '/content/drive/MyDrive/Colab Notebooks/testhindistatements.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDa3RA3WukcC",
        "trusted": true
      },
      "source": [
        "import csv\n",
        "\n",
        "finaldata=[]\n",
        "\n",
        "with open(testpath, 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for line in reader:\n",
        "        finaldata.append(line['hindi'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I53_hsmZSi9B"
      },
      "source": [
        "### **Store the corresponding Translated English Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88dzuUTlHmd1",
        "trusted": true
      },
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/FinalRound7/answer.txt', 'w') as f:\n",
        "    for sent in finaldata:\n",
        "        prediction = translate_sentence(model, sent, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "        x = ' '.join([e for e in prediction])\n",
        "        f.write(x+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYiI_qjeHNfc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tsnf65FSi9C"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua81qvz2mt4U"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    }
  ]
}