{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phase1_NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUf1lKg4FwpL"
      },
      "source": [
        "## **Seq2Seq Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXpa8cYz2-ih"
      },
      "source": [
        "filepath = '/content/drive/MyDrive/Colab Notebooks/train.csv'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WthDc8jb5Nua"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6p5vDDo5HOW"
      },
      "source": [
        "data = pd.read_csv(filepath)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "o3rLtGks5Kz7",
        "outputId": "f2c37991-0f5b-4d13-865e-5dcd0e36cab9"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध...</td>\n",
              "      <td>In El Salvador, both sides that withdrew from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>मैं उनके साथ कोई लेना देना नहीं है.</td>\n",
              "      <td>I have nothing to do with them.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-हटाओ रिक.</td>\n",
              "      <td>Fuck them, Rick.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>क्योंकि यह एक खुशियों भरी फ़िल्म है.</td>\n",
              "      <td>Because it's a happy film.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>The thought reaching the eyes...</td>\n",
              "      <td>The thought reaching the eyes...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                            english\n",
              "0           0  ...  In El Salvador, both sides that withdrew from ...\n",
              "1           1  ...                    I have nothing to do with them.\n",
              "2           2  ...                                   Fuck them, Rick.\n",
              "3           3  ...                         Because it's a happy film.\n",
              "4           4  ...                   The thought reaching the eyes...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC5mkpvL5cda",
        "outputId": "5d9eea14-6ccb-4fb0-c475-8d15677661fe"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102322, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elcDCdku5h90",
        "outputId": "ec84c8a2-9a62-4f21-8b71-af422cce49f2"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'hindi', 'english'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB04pL7T6Plt"
      },
      "source": [
        "data.drop(columns='Unnamed: 0', inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "AFDx2t7WF-tb",
        "outputId": "5d2c0465-5a94-47a3-83b2-23b1f0c74752"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध...</td>\n",
              "      <td>In El Salvador, both sides that withdrew from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>मैं उनके साथ कोई लेना देना नहीं है.</td>\n",
              "      <td>I have nothing to do with them.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-हटाओ रिक.</td>\n",
              "      <td>Fuck them, Rick.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>क्योंकि यह एक खुशियों भरी फ़िल्म है.</td>\n",
              "      <td>Because it's a happy film.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The thought reaching the eyes...</td>\n",
              "      <td>The thought reaching the eyes...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               hindi                                            english\n",
              "0  एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध...  In El Salvador, both sides that withdrew from ...\n",
              "1                मैं उनके साथ कोई लेना देना नहीं है.                    I have nothing to do with them.\n",
              "2                                         -हटाओ रिक.                                   Fuck them, Rick.\n",
              "3               क्योंकि यह एक खुशियों भरी फ़िल्म है.                         Because it's a happy film.\n",
              "4                   The thought reaching the eyes...                   The thought reaching the eyes..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJJVSS1bB6qV"
      },
      "source": [
        "#Ref: https://towardsdatascience.com/how-to-split-a-dataframe-into-train-and-test-set-with-python-eaa1630ca7b3\n",
        "        \n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSvXCbFKCBeT"
      },
      "source": [
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "tqNTTwI6CIsT",
        "outputId": "11732d3c-da27-487e-b8c0-80081f560e64"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>मैं पिता की तरह तुम राजा हूं, कब?</td>\n",
              "      <td>When I'm king like you, Father?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>उत्तरी डकोटा, Standing Rock Nation इस कछुओं के...</td>\n",
              "      <td>North Dakota. Standing Rock Nation, in this Tu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>यह आवाज के साथ इश्क कर रही है.</td>\n",
              "      <td>It's flirting with sound.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>सुधार, वे मेरे लिए जवाब देना होगा .</td>\n",
              "      <td>Correction, they will have to answer to me.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>काफ़ी लोगों नें कहा, \"ऐसा है, शहरी लोग कौन होते...</td>\n",
              "      <td>You know, a lot of people said, \"Well, you kno...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               hindi                                            english\n",
              "0                  मैं पिता की तरह तुम राजा हूं, कब?                    When I'm king like you, Father?\n",
              "1  उत्तरी डकोटा, Standing Rock Nation इस कछुओं के...  North Dakota. Standing Rock Nation, in this Tu...\n",
              "2                     यह आवाज के साथ इश्क कर रही है.                          It's flirting with sound.\n",
              "3                सुधार, वे मेरे लिए जवाब देना होगा .        Correction, they will have to answer to me.\n",
              "4  काफ़ी लोगों नें कहा, \"ऐसा है, शहरी लोग कौन होते...  You know, a lot of people said, \"Well, you kno..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "WoOQBFchCLTT",
        "outputId": "f7e93bef-828c-4efc-f6fa-31c985069148"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>चिन अप, बर्ट.</td>\n",
              "      <td>Chin up, Burt.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>कैदियों की तरह देखते थे। उनको बेचने से राजा अप...</td>\n",
              "      <td>By selling them, kings enriched their own real...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>केवट ऑलसेन!</td>\n",
              "      <td>It's boatsman Olsen!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>निस्संदेह रूढ़िवादी हमेशा गलत होते हैं, ¶ लेकि...</td>\n",
              "      <td>Stereotypes are always a mistake, of course, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>अगर वो कर सकते हैं, तो सरकारें और गैर-लाभ संस्...</td>\n",
              "      <td>If they can do that, why can't governments and...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               hindi                                            english\n",
              "0                                      चिन अप, बर्ट.                                     Chin up, Burt.\n",
              "1  कैदियों की तरह देखते थे। उनको बेचने से राजा अप...  By selling them, kings enriched their own real...\n",
              "2                                        केवट ऑलसेन!                               It's boatsman Olsen!\n",
              "3  निस्संदेह रूढ़िवादी हमेशा गलत होते हैं, ¶ लेकि...  Stereotypes are always a mistake, of course, b...\n",
              "4  अगर वो कर सकते हैं, तो सरकारें और गैर-लाभ संस्...  If they can do that, why can't governments and..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbzP2TRmWhuN",
        "outputId": "659a10d1-af52-4b0e-ac64-26a403ccde77"
      },
      "source": [
        "train.shape[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QGP4WdkGXaLL",
        "outputId": "b956ed5b-44c6-478c-aeb2-f00b061e85b9"
      },
      "source": [
        "train.loc[0,'hindi']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'मैं पिता की तरह तुम राजा हूं, कब?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyLVdNxtU6IS"
      },
      "source": [
        "#Create train hindi file and train english file\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/train_hindi.txt', 'w') as f:\n",
        "  for i in range(train.shape[0]):\n",
        "    f.write(train.iloc[i]['hindi']+'\\n')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd0QQsl2ZU0w"
      },
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/train_english.txt', 'w') as f:\n",
        "  for i in range(train.shape[0]):\n",
        "    f.write(train.iloc[i]['english']+'\\n')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7wc52cICPF5"
      },
      "source": [
        "# make sentence list for training data\n",
        "\n",
        "hindi_sentence_list = train['hindi'].to_list()\n",
        "english_sentence_list = train['english'].to_list()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKX5ZOc0dRVx"
      },
      "source": [
        "# make test sentence list\n",
        "\n",
        "\n",
        "hindi_test_sentence_list = test['hindi'].to_list()\n",
        "english_test_sentence_list = test['english'].to_list()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Szd8B3dcPYfT",
        "outputId": "08319103-22d1-458d-fb7d-4a7d967d30e4"
      },
      "source": [
        "len(hindi_sentence_list), len(english_sentence_list)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(81857, 81857)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXdg-U-ldeoo",
        "outputId": "5bdf603b-e7b0-47c2-fc2c-adf8861fb171"
      },
      "source": [
        "len(hindi_test_sentence_list), len(english_test_sentence_list)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20465, 20465)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhwpdMlDF8vv"
      },
      "source": [
        "### **Install Indic NLP Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph3H3x0zDe2h",
        "outputId": "8b978f20-bfa7-40b0-dacc-a1486909aabe"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1325, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 1325 (delta 84), reused 89 (delta 41), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1325/1325), 9.57 MiB | 10.89 MiB/s, done.\n",
            "Resolving deltas: 100% (688/688), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM1dnvG6Dap1",
        "outputId": "8d7aa08f-d76a-44b8-9a3c-378f9e655809"
      },
      "source": [
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (133/133), 149.77 MiB | 33.16 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRJ0Hd7nEc1H",
        "outputId": "8dd4f5f6-63a6-467a-8975-e8b06167c395"
      },
      "source": [
        "!pip install Morfessor"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLa9zj92EglQ"
      },
      "source": [
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIn-N_z0EnVC"
      },
      "source": [
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBvcuzn_ErXf"
      },
      "source": [
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43YadMNsEu1g"
      },
      "source": [
        "from indicnlp import loader\n",
        "loader.load()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOKGcupGGPvD"
      },
      "source": [
        "### **Tokenize Hindi sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CFni1UAG9y9"
      },
      "source": [
        "# Detokenize\n",
        "# Ref: https://colab.research.google.com/drive/1p3oGPcNdORw5_MDcufTDYWJhJt3XVPuC?usp=sharing#scrollTo=GU6E07Yw5zvl\n",
        "\n",
        "from indicnlp.tokenize import indic_detokenize  \n",
        "\n",
        "for i in range(len(hindi_sentence_list)):\n",
        "  hindi_sentence_list[i] = indic_detokenize.trivial_detokenize(hindi_sentence_list[i],lang='hi')\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ytzhhOAqyY-R",
        "outputId": "f5afa3dc-9553-4b70-a8ff-4d87c56f53c2"
      },
      "source": [
        "import string \n",
        "string.punctuation"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQdSsjw8Nqwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8082cbfd-6e70-4f0b-d29e-5bbfff4ca685"
      },
      "source": [
        "#Ref for unicode chart: https://www.ssec.wisc.edu/~tomw/java/unicode.html#x0900 \n",
        "\n",
        "import re\n",
        "\n",
        "from indicnlp.tokenize import indic_tokenize \n",
        "\n",
        "hindi_word_to_count={}\n",
        "hindi_word_to_index={'UNK':0, 'PAD':1, 'SOS':2, 'EOS':3}\n",
        "hindi_index_to_word={0:'UNK', 1:'PAD', 2:'SOS', 3:'EOS'}\n",
        "count=4\n",
        "for sent in hindi_sentence_list:\n",
        "  for t in indic_tokenize.trivial_tokenize(sent): \n",
        "    x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "    for elem in x:\n",
        "      hindi_word_to_count[elem] = hindi_word_to_count.get(elem,0)+1\n",
        "      if hindi_word_to_index.get(elem) is None and hindi_word_to_count.get(elem,0) >= 2:\n",
        "        hindi_word_to_index[elem] = count\n",
        "        hindi_index_to_word[count] = elem\n",
        "        count+=1\n",
        "print(count)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdfXWlPzjzyM"
      },
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/HindiWordToCount.txt','w') as f:\n",
        "  for k,v in hindi_word_to_count.items():\n",
        "    f.write(str(k)+\",\"+str(v)+\"\\n\")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kohp2FyE_A1F",
        "outputId": "a88a471d-c86d-4e24-ef50-fb6fd0e5ed69"
      },
      "source": [
        "print(len(hindi_word_to_count))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39920\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nIDutUfKbxa",
        "outputId": "42e9dee8-e386-44cc-9240-4d28966d5cdd"
      },
      "source": [
        "print(len(hindi_word_to_index))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSfh63mXqitj"
      },
      "source": [
        "### **Tokenize English sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGb6O4X1oTyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed441cd-4c51-43f7-a79b-4807a3959a53"
      },
      "source": [
        "!python3 -m spacy download en"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.2.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn6zkR4-bctJ"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-tXvjpCiD29"
      },
      "source": [
        "# english_word_to_count={}\n",
        "# english_word_to_index={'UNK':0, 'PAD':1, 'SOS':2, 'EOS':3}\n",
        "# english_index_to_word={0:'UNK', 1:'PAD', 2:'SOS', 3:'EOS'}\n",
        "# count=4\n",
        "\n",
        "# for sent in english_sentence_list:\n",
        "#   for token in nlp.tokenizer(sent.lower()):\n",
        "#     temp = re.findall('[A-Za-z\\']+', token.text)\n",
        "#     for elem in temp:\n",
        "#       english_word_to_count[elem] = english_word_to_count.get(elem,0)+1\n",
        "#       if english_word_to_index.get(elem) is None and english_word_to_count.get(elem,0) >= 2:\n",
        "#         english_word_to_index[elem] = count\n",
        "#         english_index_to_word[count] = elem\n",
        "#         count+=1\n",
        "# print(count)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCGsCkw01Po3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5304bde-9fda-4736-df97-1eee41fc74de"
      },
      "source": [
        "english_word_to_count={}\n",
        "english_word_to_index={'UNK':0, 'PAD':1, 'SOS':2, 'EOS':3}\n",
        "english_index_to_word={0:'UNK', 1:'PAD', 2:'SOS', 3:'EOS'}\n",
        "count=4\n",
        "\n",
        "for sent in english_sentence_list:\n",
        "  for token in nlp.tokenizer(sent.lower()):\n",
        "    temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token.text)\n",
        "    for elem in temp:\n",
        "      english_word_to_count[elem] = english_word_to_count.get(elem,0)+1\n",
        "      if english_word_to_index.get(elem) is None and english_word_to_count.get(elem,0) >= 2:\n",
        "        english_word_to_index[elem] = count\n",
        "        english_index_to_word[count] = elem\n",
        "        count+=1\n",
        "print(count)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWeSR4fzth6M"
      },
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/EnglishWordToCount.txt','w') as f:\n",
        "  for k,v in english_word_to_count.items():\n",
        "    f.write(str(k)+\",\"+str(v)+\"\\n\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJvOrOpvefHa"
      },
      "source": [
        "### **Find Sentences Length and fix the maximum length and filter the sentences**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-ziiOx1ko9j"
      },
      "source": [
        "First Let's Check for Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s49M8J3ed4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4624dad-4bda-4881-8a39-16a4b7c3a70d"
      },
      "source": [
        "sent_len_count={}\n",
        "for sent in hindi_sentence_list:\n",
        "  sent_len=0\n",
        "  for t in indic_tokenize.trivial_tokenize(sent): \n",
        "    x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "    for elem in x:\n",
        "      sent_len+=1\n",
        "  sent_len_count[sent_len] = sent_len_count.get(sent_len,0)+1\n",
        "\n",
        "print(sent_len_count)\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{10: 4142, 36: 266, 9: 5068, 28: 645, 3: 3866, 8: 5652, 7: 6170, 26: 731, 2: 2751, 13: 2763, 24: 847, 31: 453, 37: 267, 5: 6383, 6: 6487, 22: 1051, 25: 774, 14: 2383, 29: 558, 11: 3643, 4: 5620, 12: 3206, 17: 1628, 21: 1123, 30: 527, 19: 1342, 16: 1959, 23: 932, 20: 1282, 18: 1527, 32: 424, 15: 2163, 46: 113, 34: 367, 27: 685, 44: 141, 87: 3, 42: 187, 33: 415, 85: 8, 41: 177, 35: 354, 49: 90, 53: 67, 57: 29, 51: 89, 80: 8, 52: 73, 60: 38, 55: 55, 62: 26, 58: 44, 596: 1, 59: 32, 38: 257, 47: 111, 50: 95, 1: 324, 66: 26, 43: 194, 74: 22, 39: 216, 40: 222, 45: 146, 67: 21, 54: 52, 56: 49, 76: 6, 68: 27, 64: 32, 72: 19, 70: 10, 79: 8, 48: 117, 61: 47, 69: 19, 78: 5, 0: 3, 139: 1, 63: 22, 94: 2, 65: 21, 82: 4, 116: 1, 109: 4, 92: 2, 84: 8, 86: 9, 83: 10, 112: 1, 90: 3, 95: 3, 77: 10, 73: 10, 99: 4, 81: 5, 144: 1, 71: 15, 75: 11, 107: 2, 125: 1, 88: 3, 118: 3, 93: 1, 100: 1, 104: 4, 91: 2, 97: 2, 106: 1, 127: 1, 111: 1, 102: 1, 96: 4, 133: 1, 131: 1, 165: 1, 105: 2, 223: 1, 141: 1, 101: 3, 113: 1, 98: 2, 138: 1, 89: 3, 122: 2, 180: 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d5Pti5FjYSV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cae02ab-b159-4ac1-eecc-7d80a5c8a03d"
      },
      "source": [
        "# sort the dictionary based on their counts\n",
        "import operator\n",
        "\n",
        "sorted_counts = sorted(sent_len_count.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "print(sorted_counts)\n",
        "index=0\n",
        "for pair in sorted_counts:\n",
        "  if pair[1]>300:\n",
        "    index+=1\n",
        "  else:\n",
        "    break\n",
        "\n",
        "max_hindi_len=0\n",
        "for pair in sorted_counts[:index]:\n",
        "  if pair[0]>max_hindi_len:\n",
        "    max_hindi_len=pair[0]\n",
        "\n",
        "print(max_hindi_len)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(6, 6487), (5, 6383), (7, 6170), (8, 5652), (4, 5620), (9, 5068), (10, 4142), (3, 3866), (11, 3643), (12, 3206), (13, 2763), (2, 2751), (14, 2383), (15, 2163), (16, 1959), (17, 1628), (18, 1527), (19, 1342), (20, 1282), (21, 1123), (22, 1051), (23, 932), (24, 847), (25, 774), (26, 731), (27, 685), (28, 645), (29, 558), (30, 527), (31, 453), (32, 424), (33, 415), (34, 367), (35, 354), (1, 324), (37, 267), (36, 266), (38, 257), (40, 222), (39, 216), (43, 194), (42, 187), (41, 177), (45, 146), (44, 141), (48, 117), (46, 113), (47, 111), (50, 95), (49, 90), (51, 89), (52, 73), (53, 67), (55, 55), (54, 52), (56, 49), (61, 47), (58, 44), (60, 38), (59, 32), (64, 32), (57, 29), (68, 27), (62, 26), (66, 26), (74, 22), (63, 22), (67, 21), (65, 21), (72, 19), (69, 19), (71, 15), (75, 11), (70, 10), (83, 10), (77, 10), (73, 10), (86, 9), (85, 8), (80, 8), (79, 8), (84, 8), (76, 6), (78, 5), (81, 5), (82, 4), (109, 4), (99, 4), (104, 4), (96, 4), (87, 3), (0, 3), (90, 3), (95, 3), (88, 3), (118, 3), (101, 3), (89, 3), (94, 2), (92, 2), (107, 2), (91, 2), (97, 2), (105, 2), (98, 2), (122, 2), (596, 1), (139, 1), (116, 1), (112, 1), (144, 1), (125, 1), (93, 1), (100, 1), (106, 1), (127, 1), (111, 1), (102, 1), (133, 1), (131, 1), (165, 1), (223, 1), (141, 1), (113, 1), (138, 1), (180, 1)]\n",
            "35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZQe8m3kmEyg"
      },
      "source": [
        "Now Let's check for English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9LgNSHg21Fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b87c95c1-fc12-433c-eb49-cf4c53489db8"
      },
      "source": [
        "english_sent_len_count={}\n",
        "for sent in english_sentence_list:\n",
        "  sent_len=0\n",
        "  for token in nlp.tokenizer(sent.lower()): \n",
        "    temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token.text)\n",
        "    for elem in temp:\n",
        "      sent_len+=1\n",
        "  english_sent_len_count[sent_len] = english_sent_len_count.get(sent_len,0)+1\n",
        "\n",
        "print(english_sent_len_count)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{9: 4922, 32: 418, 6: 7294, 10: 4244, 31: 445, 15: 1973, 3: 3970, 7: 6587, 28: 559, 24: 767, 2: 2972, 11: 3428, 19: 1263, 16: 1838, 35: 272, 4: 6229, 8: 5701, 20: 1119, 5: 7095, 25: 714, 13: 2679, 12: 3145, 14: 2318, 17: 1590, 22: 968, 30: 465, 27: 602, 33: 341, 43: 143, 29: 529, 18: 1424, 44: 135, 92: 3, 37: 210, 55: 40, 21: 1026, 23: 895, 26: 659, 42: 157, 49: 78, 60: 27, 38: 238, 80: 8, 47: 90, 50: 74, 64: 20, 51: 65, 446: 1, 34: 318, 39: 185, 1: 100, 36: 267, 79: 7, 46: 99, 48: 90, 40: 192, 45: 111, 88: 6, 52: 60, 90: 4, 41: 164, 65: 20, 58: 29, 56: 40, 54: 51, 53: 42, 74: 9, 69: 15, 63: 14, 128: 1, 82: 3, 61: 20, 59: 32, 57: 39, 68: 10, 76: 8, 66: 17, 73: 7, 77: 6, 70: 13, 94: 2, 67: 15, 71: 15, 78: 6, 62: 23, 72: 11, 89: 6, 86: 6, 83: 4, 99: 3, 84: 6, 100: 1, 87: 4, 75: 7, 81: 2, 85: 2, 0: 1, 107: 1, 95: 2, 108: 1, 106: 3, 93: 1, 116: 1, 110: 1, 157: 1, 104: 1, 261: 1, 143: 1, 114: 1, 123: 1, 121: 1, 103: 1, 96: 1, 91: 2, 112: 1, 164: 1, 282: 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEOBhAxDc8gs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a626d7-32df-415b-8fb7-069542540928"
      },
      "source": [
        "# sort the dictionary based on their counts\n",
        "import operator\n",
        "\n",
        "english_sorted_counts = sorted(english_sent_len_count.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "index=0\n",
        "for pair in english_sorted_counts:\n",
        "  if pair[1]>300:\n",
        "    index+=1\n",
        "  else:\n",
        "    break\n",
        "\n",
        "print(index)\n",
        "max_english_len=0\n",
        "for pair in english_sorted_counts[:index]:\n",
        "  if pair[0]>max_english_len:\n",
        "    max_english_len=pair[0]\n",
        "\n",
        "print(max_english_len)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33\n",
            "34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCGccsRC-QVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "708f6acb-a237-4735-8765-8655d701b0c8"
      },
      "source": [
        "max_len = max(max_hindi_len, max_english_len)\n",
        "max_len"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAbUhu62u18-"
      },
      "source": [
        "### **Filter out sentences with length less than or equal to maximum length**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP5Vr6pfu1AV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c6ee9a-b536-4176-8612-19c2431210d6"
      },
      "source": [
        "english_filtered_sent_list=[]\n",
        "hindi_filtered_sent_list=[]\n",
        "filtered_sent_pair_list=[]\n",
        "for hin_sent, eng_sent in zip(hindi_sentence_list, english_sentence_list):\n",
        "  hin_sent_len=0\n",
        "  for t in indic_tokenize.trivial_tokenize(hin_sent): \n",
        "    x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "    for elem in x:\n",
        "      hin_sent_len+=1\n",
        "\n",
        "  eng_sent_len=0\n",
        "  for token in nlp.tokenizer(eng_sent.lower()): \n",
        "    temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token.text)\n",
        "    for elem in temp:\n",
        "      eng_sent_len+=1\n",
        "\n",
        "  if hin_sent_len>=1 and hin_sent_len<=max_len and eng_sent_len>=1 and eng_sent_len<=max_len:\n",
        "    english_filtered_sent_list.append(eng_sent)\n",
        "    hindi_filtered_sent_list.append(hin_sent)\n",
        "    filtered_sent_pair_list.append([hin_sent, eng_sent])\n",
        "\n",
        "print(len(english_filtered_sent_list))\n",
        "print(len(hindi_filtered_sent_list))\n",
        "print(len(filtered_sent_pair_list))\n",
        "print(filtered_sent_pair_list[:5])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "77825\n",
            "77825\n",
            "77825\n",
            "[['मैं पिता की तरह तुम राजा हूं, कब?', \"When I'm king like you, Father?\"], ['यह आवाज के साथ इश्क कर रही है.', \"It's flirting with sound.\"], ['सुधार, वे मेरे लिए जवाब देना होगा.', 'Correction, they will have to answer to me.'], ['काफ़ी लोगों नें कहा, \"ऐसा है, शहरी लोग कौन होते हैं हम ग्रामीण लोगों को बताने बाले कि अपने समय के साथ क्या करें.', 'You know, a lot of people said, \"Well, you know, city boys have no business telling us rural types what to do with our time.'], ['और तुम चूसोगे जो मैं तुम्हें चूसने दूंगा.', \".? and you're gonna swallow what I give you to swallow.\"]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pymFPol0wcqR"
      },
      "source": [
        "# Store the filtered hindi sentences in a file\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/FilteredSentencesPair.txt','w') as f:\n",
        "  for line in filtered_sent_pair_list:\n",
        "    f.write(line[0]+','+line[1]+'\\n')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DciCZiuq3P8O"
      },
      "source": [
        "### Form List of list of indexes from the filtered sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wiHp1iY4dir"
      },
      "source": [
        "valid_eng_len=[]\n",
        "valid_hin_len=[]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56Dc9KvM3NOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6201d6-ad32-4d49-c54b-c011c099eb67"
      },
      "source": [
        "# Create tensor array for each hindi sentence \n",
        "import torch\n",
        "hindi_list_indices = torch.tensor([[1]*(max_len+2)]*len(filtered_sent_pair_list), dtype=torch.long)\n",
        "\n",
        "i=0\n",
        "for sent in hindi_filtered_sent_list:\n",
        "  hindi_list_indices[i][0]=2  #SOS\n",
        "  j=1\n",
        "  for t in indic_tokenize.trivial_tokenize(sent): \n",
        "    x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "    for elem in x:\n",
        "      if hindi_word_to_index.get(elem) is None:\n",
        "        continue\n",
        "        #hindi_list_indices[i][j] = 0  # UNK\n",
        "      else:\n",
        "        hindi_list_indices[i][j] = hindi_word_to_index.get(elem)\n",
        "      j+=1\n",
        "  hindi_list_indices[i][j]=3  #EOS\n",
        "  valid_hin_len.append(j)\n",
        "  j+=1\n",
        "  while j<=(max_len+1):\n",
        "    hindi_list_indices[i][j] = 1  #PAD\n",
        "    j+=1\n",
        "\n",
        "  i+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create tensor array for each hindi test sentence\n",
        "hindi_test_list_indices = torch.tensor([[1]*(max_len+2)]*len(hindi_test_sentence_list), dtype=torch.long)\n",
        "\n",
        "i=0\n",
        "for sent in hindi_test_sentence_list:\n",
        "  hindi_test_list_indices[i][0]=2  #SOS\n",
        "  j=1\n",
        "  for t in indic_tokenize.trivial_tokenize(sent): \n",
        "    x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "    for elem in x:\n",
        "      if j>(max_len):\n",
        "        break\n",
        "      if hindi_word_to_index.get(elem) is None:\n",
        "        continue\n",
        "        #hindi_test_list_indices[i][j] = 0  # UNK\n",
        "      else:\n",
        "        hindi_test_list_indices[i][j] = hindi_word_to_index.get(elem)\n",
        "      j+=1\n",
        "  hindi_test_list_indices[i][j]=3  #EOS\n",
        "  j+=1\n",
        "  while j<=(max_len+1):\n",
        "    hindi_test_list_indices[i][j] = 1  #PAD\n",
        "    j+=1\n",
        "\n",
        "  i+=1\n",
        "\n",
        "\n",
        "# Create tensor array for each english sentence\n",
        "\n",
        "english_list_indices = torch.tensor([[1]*(max_len+2)]*len(filtered_sent_pair_list), dtype=torch.long)\n",
        "\n",
        "i=0\n",
        "for sent in english_filtered_sent_list:\n",
        "  english_list_indices[i][0]=2  #SOS\n",
        "  j=1\n",
        "  for token in nlp.tokenizer(sent.lower()):\n",
        "    temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token.text)\n",
        "    for elem in temp:\n",
        "      if english_word_to_index.get(elem) is None:\n",
        "        continue\n",
        "        #english_list_indices[i][j] = 0  # UNK\n",
        "      else:\n",
        "        english_list_indices[i][j] = english_word_to_index.get(elem)\n",
        "      j+=1\n",
        "  english_list_indices[i][j]=3  #EOS\n",
        "  valid_eng_len.append(j)\n",
        "  j+=1\n",
        "  while j<=(max_len+1):\n",
        "    english_list_indices[i][j] = 1  #PAD\n",
        "    j+=1\n",
        "  \n",
        "  i+=1\n",
        "\n",
        "\n",
        "# Create tensor array for each english test sentence\n",
        "english_test_list_indices = torch.tensor([[1]*(max_len+2)]*len(english_test_sentence_list), dtype=torch.long)\n",
        "\n",
        "i=0\n",
        "for sent in english_test_sentence_list:\n",
        "  english_test_list_indices[i][0]=2  #SOS\n",
        "  j=1\n",
        "  for token in nlp.tokenizer(sent.lower()):\n",
        "    temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token.text)\n",
        "    for elem in temp:\n",
        "      if j>(max_len):\n",
        "        break\n",
        "      if english_word_to_index.get(elem) is None:\n",
        "        continue\n",
        "        #english_test_list_indices[i][j] = 0  # UNK\n",
        "      else:\n",
        "        english_test_list_indices[i][j] = english_word_to_index.get(elem)\n",
        "      j+=1\n",
        "  english_test_list_indices[i][j]=3  #EOS\n",
        "  j+=1\n",
        "  while j<=(max_len+1):\n",
        "    english_test_list_indices[i][j] = 1  #PAD\n",
        "    j+=1\n",
        "  \n",
        "  i+=1\n",
        "\n",
        "print(hindi_list_indices[:5])\n",
        "print(english_list_indices[:5])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[    2,    17,    44,    22,    48,    16,   785,    41,     4,  1009,\n",
            "            21,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1],\n",
            "        [    2,    29,   227,     8,    15,   402,    40,    67,    10,     9,\n",
            "             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1],\n",
            "        [    2,  1156,     4,    49,    58,    20,   707,   217,   306,     9,\n",
            "             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1],\n",
            "        [    2,   584,    13,  2716,   202,     4,    31,   148,    10,     4,\n",
            "           426,    11,   144,    92,    12,    38,   494,    13,    37,   731,\n",
            "          8824,    14,    82,   109,     8,    15,    65,   142,     9,     3,\n",
            "             1,     1,     1,     1,     1,     1,     1],\n",
            "        [    2,    26,    16, 18244,    47,    17,   106, 14291,  2382,     9,\n",
            "             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1]])\n",
            "tensor([[   2,   50,   15,   59,  704,   53,    9,    4,   74,   13,    3,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1],\n",
            "        [   2,   42,   56, 4669,   12, 1303,    7,    3,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1],\n",
            "        [   2,    4,   97,  269,   11,    8,  636,    8,   65,    7,    3,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1],\n",
            "        [   2,    9,   10,    4,   26,  255,   18,   32,  123,    4,   27,   29,\n",
            "            4,    9,   10,    4,  364,  301,   11,   17,  119,  337,  126,  431,\n",
            "         1305,   14,    8,   25,   12,   69,   95,    7,    3,    1,    1,    1,\n",
            "            1],\n",
            "        [   2,    7,   13,   22,    9,   55,  262,  263,   16,   14,   15,   83,\n",
            "            9,    8,   16,    7,    3,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leztTC8ziQmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5720ed48-9400-4d5b-b5b0-b08ba59e6ff6"
      },
      "source": [
        "print(hindi_list_indices.shape)\n",
        "print(english_list_indices.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77825, 37])\n",
            "torch.Size([77825, 37])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmJ6InY6iSuv"
      },
      "source": [
        "### Zip The English and hindi text and process it simultaneously"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RM5t4w5-kkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3debcb9-1d38-4282-d645-2d500f70461a"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "print(device)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0OWOYmqCpwm"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiDTgw5014t_"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, embedding_size, hidden_dimension, num_layers, dropout):  \n",
        "    super().__init__()\n",
        "    \n",
        "    self.hidden_dimension = hidden_dimension\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = dropout\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_dimension, num_layers, dropout = dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "  def forward(self, word_inputs):\n",
        "  \n",
        "    embedded = self.dropout(self.embedding(word_inputs))\n",
        "    \n",
        "    #embedded = [batch_size, sequence_length, embedding_size]\n",
        "    \n",
        "    outputs, (hidden, cell) = self.rnn(embedded)\n",
        "    \n",
        "    return hidden, cell"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdKil5S03aZa"
      },
      "source": [
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_size, embedding_size, hidden_dimension, num_layers, dropout):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.output_size = output_size\n",
        "    self.hidden_dimension = hidden_dimension\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "    \n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_dimension, num_layers, dropout = dropout)\n",
        "    \n",
        "    self.last_layer = nn.Linear(hidden_dimension, output_size)\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "  def forward(self, input, hidden, cell):\n",
        "    \n",
        "    input = input.unsqueeze(0)\n",
        "\n",
        "    embedded = self.dropout(self.embedding(input))  \n",
        "    \n",
        "    output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "    \n",
        "    prediction = self.last_layer(output.squeeze(0))\n",
        "\n",
        "    return prediction, hidden, cell"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaGtBf8Xv0Xz"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "      \n",
        "  def forward(self, source, target, teacher_force = 1):\n",
        "\n",
        "    batch_size = target.shape[1]\n",
        "    sequence_length = target.shape[0]\n",
        "\n",
        "    target_dict_size = self.decoder.output_size\n",
        "    \n",
        "    pred_output = torch.zeros(sequence_length, batch_size, target_dict_size).to(self.device)\n",
        "    \n",
        "    hidden, cell = self.encoder(source)\n",
        "    \n",
        "    input = target[0]\n",
        "    \n",
        "    for i in range(1, sequence_length):\n",
        "\n",
        "      output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "      \n",
        "      pred_output[i] = output\n",
        "    \n",
        "      best_pred = output.argmax(1) \n",
        "\n",
        "      if random.random() < teacher_force:\n",
        "        input = target[i]\n",
        "\n",
        "      else:\n",
        "        input = best_pred\n",
        "    \n",
        "    return pred_output\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvECVNV77wFQ"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqoT3XxmwqZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777dc3db-2b89-4de4-a14f-3bfe4af7d188"
      },
      "source": [
        "input_size = len(hindi_word_to_index)\n",
        "output_size = len(english_word_to_index)\n",
        "embedding_size = 256\n",
        "hidden_dimension = 512\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "\n",
        "enc = Encoder(input_size, embedding_size, hidden_dimension, num_layers, dropout).to(device)\n",
        "dec = Decoder(output_size, embedding_size, hidden_dimension, num_layers, dropout).to(device)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "print(enc)\n",
        "print(dec)\n",
        "print(model)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder(\n",
            "  (embedding): Embedding(19168, 256)\n",
            "  (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(16715, 256)\n",
            "  (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
            "  (last_layer): Linear(in_features=512, out_features=16715, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(19168, 256)\n",
            "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(16715, 256)\n",
            "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
            "    (last_layer): Linear(in_features=512, out_features=16715, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMaINRf5cpi"
      },
      "source": [
        "### **Test Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXy9rWYl7ugV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b2e4a6-3d1c-454f-98c1-21b7cc849df7"
      },
      "source": [
        "word_input = torch.zeros((7, 4), dtype=torch.long, device=device)  # here 7 is seq length and 4 is batch size\n",
        "print(word_input.shape)\n",
        "enc_hid, enc_cell = enc.forward(word_input)  # encode this word_input\n",
        "\n",
        "print(enc_hid)\n",
        "print(enc_cell)\n",
        "\n",
        "print(enc_hid.shape) # [num_layers, seq_length, hidden_units]\n",
        "print(enc_cell.shape) # [num_layers, seq_length, hidden_units]  Here hidden_units mean hidden dimension"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([7, 4])\n",
            "tensor([[[ 0.1904,  0.1793, -0.2109,  ...,  0.0713, -0.0887, -0.0097],\n",
            "         [ 0.1411,  0.1646, -0.0862,  ...,  0.1745, -0.0210, -0.0557],\n",
            "         [ 0.2216,  0.0308,  0.0233,  ...,  0.1827, -0.0080, -0.1731],\n",
            "         [ 0.3838, -0.0009,  0.1093,  ...,  0.2471,  0.0230, -0.1818]],\n",
            "\n",
            "        [[ 0.1378, -0.0015, -0.0041,  ...,  0.0698, -0.0034, -0.0540],\n",
            "         [ 0.1190, -0.0130,  0.0690,  ...,  0.0238,  0.1155, -0.0479],\n",
            "         [ 0.1149, -0.0104, -0.0211,  ...,  0.0702,  0.0239, -0.0659],\n",
            "         [ 0.0835, -0.0350,  0.0783,  ...,  0.0827,  0.0294, -0.1209]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
            "tensor([[[ 0.4157,  0.4014, -0.3680,  ...,  0.1384, -0.2407, -0.0169],\n",
            "         [ 0.2526,  0.2766, -0.1569,  ...,  0.4593, -0.0976, -0.0791],\n",
            "         [ 0.4453,  0.1030,  0.0349,  ...,  0.2475, -0.0348, -0.2767],\n",
            "         [ 0.6123, -0.0021,  0.2198,  ...,  0.5197,  0.0948, -0.3113]],\n",
            "\n",
            "        [[ 0.2732, -0.0030, -0.0079,  ...,  0.1375, -0.0067, -0.1185],\n",
            "         [ 0.2466, -0.0250,  0.1597,  ...,  0.0496,  0.2234, -0.1115],\n",
            "         [ 0.2226, -0.0192, -0.0452,  ...,  0.1486,  0.0509, -0.1433],\n",
            "         [ 0.1767, -0.0673,  0.1640,  ...,  0.1640,  0.0589, -0.2510]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
            "torch.Size([2, 4, 512])\n",
            "torch.Size([2, 4, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2eHI-7VB1tY"
      },
      "source": [
        "### **Test Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5RUUFp-_9WA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf803650-9a0e-41c8-a82c-6295571607ed"
      },
      "source": [
        "for i in range(7):\n",
        "    input = word_input[i]\n",
        "    pred, dec_hid, dec_cell = dec(input, enc_hid, enc_cell)\n",
        "    print(dec_hid.shape, pred, dec_cell.shape)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4, 512]) tensor([[ 0.0549,  0.0527,  0.0217,  ...,  0.0085, -0.0515, -0.0003],\n",
            "        [ 0.0386,  0.0704, -0.0025,  ..., -0.0382,  0.0241, -0.0326],\n",
            "        [ 0.0208,  0.0344,  0.0136,  ..., -0.0285,  0.0048, -0.0025],\n",
            "        [ 0.0381,  0.0222,  0.0002,  ..., -0.0158,  0.0242,  0.0256]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>) torch.Size([2, 4, 512])\n",
            "torch.Size([2, 4, 512]) tensor([[ 0.0267,  0.0124, -0.0089,  ..., -0.0288, -0.0105, -0.0036],\n",
            "        [ 0.0088,  0.0568,  0.0277,  ..., -0.0582,  0.0103, -0.0254],\n",
            "        [ 0.0339,  0.0221,  0.0033,  ..., -0.0252,  0.0120,  0.0027],\n",
            "        [ 0.0176,  0.0020,  0.0139,  ..., -0.0425,  0.0098,  0.0191]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>) torch.Size([2, 4, 512])\n",
            "torch.Size([2, 4, 512]) tensor([[ 0.0322, -0.0046, -0.0122,  ...,  0.0031, -0.0055, -0.0296],\n",
            "        [-0.0076,  0.0417,  0.0306,  ..., -0.0296, -0.0069, -0.0509],\n",
            "        [ 0.0298,  0.0544,  0.0223,  ..., -0.0057, -0.0158,  0.0041],\n",
            "        [ 0.0112,  0.0231,  0.0256,  ..., -0.0416, -0.0056,  0.0098]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>) torch.Size([2, 4, 512])\n",
            "torch.Size([2, 4, 512]) tensor([[ 0.0503,  0.0312,  0.0223,  ..., -0.0063, -0.0412, -0.0027],\n",
            "        [ 0.0141,  0.0370,  0.0244,  ..., -0.0338,  0.0110, -0.0098],\n",
            "        [ 0.0358,  0.0048, -0.0006,  ..., -0.0141, -0.0271, -0.0455],\n",
            "        [ 0.0174,  0.0241,  0.0558,  ..., -0.0422,  0.0287,  0.0055]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>) torch.Size([2, 4, 512])\n",
            "torch.Size([2, 4, 512]) tensor([[ 0.0339,  0.0258,  0.0036,  ...,  0.0033, -0.0231, -0.0383],\n",
            "        [ 0.0237,  0.0754, -0.0124,  ..., -0.0370,  0.0040, -0.0517],\n",
            "        [ 0.0485,  0.0138, -0.0024,  ..., -0.0272,  0.0010, -0.0144],\n",
            "        [ 0.0189,  0.0103,  0.0346,  ..., -0.0255, -0.0039,  0.0175]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>) torch.Size([2, 4, 512])\n",
            "torch.Size([2, 4, 512]) tensor([[ 0.0279,  0.0322,  0.0149,  ..., -0.0089, -0.0016, -0.0169],\n",
            "        [ 0.0148,  0.0442,  0.0016,  ...,  0.0040,  0.0182, -0.0406],\n",
            "        [ 0.0452,  0.0257,  0.0292,  ..., -0.0644,  0.0271,  0.0253],\n",
            "        [ 0.0180,  0.0124,  0.0040,  ..., -0.0298,  0.0004, -0.0103]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>) torch.Size([2, 4, 512])\n",
            "torch.Size([2, 4, 512]) tensor([[ 0.0157,  0.0433, -0.0002,  ..., -0.0102, -0.0279, -0.0233],\n",
            "        [ 0.0164,  0.0866,  0.0063,  ...,  0.0021, -0.0313, -0.0246],\n",
            "        [ 0.0594,  0.0414,  0.0181,  ..., -0.0002, -0.0147,  0.0187],\n",
            "        [ 0.0175, -0.0096,  0.0176,  ..., -0.0018, -0.0278, -0.0141]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>) torch.Size([2, 4, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oLUvmCQPsGD"
      },
      "source": [
        "### **Initialize Weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjLV2gluyIcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57be1b01-7603-4e8d-d6a6-722599eedfbd"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(19168, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(16715, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (last_layer): Linear(in_features=512, out_features=16715, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ieuR_dK0Jwn"
      },
      "source": [
        "### **Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Htf9iuFfAcv"
      },
      "source": [
        "from torch import optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJEV_8v24MZY"
      },
      "source": [
        "### **Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MepBgJv64Lrt"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = 1)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H9nw2k6po9a"
      },
      "source": [
        "from torch.utils import data\n",
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "    dataset = data.TensorDataset(*data_arrays)\n",
        "    return data.DataLoader(dataset, batch_size, shuffle=is_train)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKaSGeHV2wXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c077b7d-8ee5-47ed-8cc7-c7510526b9aa"
      },
      "source": [
        "len(hindi_list_indices), len(english_list_indices), type(english_list_indices)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(77825, 77825, torch.Tensor)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tgf7ovVA-kM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2b2235-0bf1-4062-e9fc-db176981b350"
      },
      "source": [
        "print(hindi_list_indices.shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77825, 37])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYB9thb3zO51"
      },
      "source": [
        "batch_size=32\n",
        "data_arrays = (hindi_list_indices, english_list_indices)\n",
        "data_iter = load_array(data_arrays, batch_size)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blPTsmfXculr"
      },
      "source": [
        "data_arrays2 = (hindi_test_list_indices, english_test_list_indices)\n",
        "valid_iter = load_array(data_arrays2, batch_size)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvXjTt4XcXqv"
      },
      "source": [
        "def translate_sentence(model, sentence, device, max_length=max_len):\n",
        "\n",
        "    sent_list=[]\n",
        "    for t in indic_tokenize.trivial_tokenize(sentence): \n",
        "      x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n",
        "      for elem in x:\n",
        "        if hindi_word_to_index.get(elem) is None:\n",
        "          #sent_list.append(hindi_word_to_index['UNK'])\n",
        "          continue\n",
        "        else:\n",
        "           sent_list.append(hindi_word_to_index[elem]) \n",
        "\n",
        "    sent_list.insert(0, hindi_word_to_index['SOS'])\n",
        "    sent_list.append(hindi_word_to_index['EOS'])\n",
        "\n",
        "    sent_tensor = torch.tensor(sent_list, dtype=torch.long).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(sent_tensor)\n",
        "\n",
        "    outputs = [2]  #SOS = 2\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        #EOS=3\n",
        "        if output.argmax(1).item() == 3:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english_index_to_word[idx] for idx in outputs]\n",
        "\n",
        "    return translated_sentence[1:]\n",
        "\n",
        "def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n",
        "    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n",
        "    torch.save(state, '/content/drive/MyDrive/Colab Notebooks/checkpoint-NMT')\n",
        "    torch.save(model.state_dict(),'/content/drive/MyDrive/Colab Notebooks/checkpoint-NMT-SD')"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTfYPg54bIL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7fc462-b17f-4471-8675-ec36c40126a4"
      },
      "source": [
        "import random\n",
        "import sys\n",
        "epoch_loss = 0.0\n",
        "num_epochs = 30\n",
        "best_loss = sys.maxsize\n",
        "best_epoch = -1\n",
        "sentence1 = \"मैं कहाँ रहते हैं आप जानते हो?\"\n",
        "step=0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"Epoch -\",epoch+1)\n",
        "  model.eval()\n",
        "  translated_sentence1 = translate_sentence(model, sentence1, device, max_length=max_len)\n",
        "\n",
        "  model.train(True)\n",
        "  for batch_idx, batch in enumerate(data_iter):\n",
        "    input, target = [x.to(device) for x in batch]\n",
        "\n",
        "    input = input.permute(1,0)\n",
        "    target = target.permute(1,0)\n",
        "\n",
        "    output = model(input, target)\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "      \n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  if epoch_loss < best_loss:\n",
        "    best_loss = epoch_loss\n",
        "    best_epoch = epoch\n",
        "    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
        "\n",
        "  print(\"Epoch_Loss - {}\".format(loss.item()))\n",
        "  print()\n",
        "  \n",
        "print(epoch_loss / len(data_iter))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch - 1\n",
            "Epoch_Loss - 5.103395938873291\n",
            "\n",
            "Epoch - 2\n",
            "Epoch_Loss - 4.9170355796813965\n",
            "\n",
            "Epoch - 3\n",
            "Epoch_Loss - 3.745488405227661\n",
            "\n",
            "Epoch - 4\n",
            "Epoch_Loss - 3.059257745742798\n",
            "\n",
            "Epoch - 5\n",
            "Epoch_Loss - 2.108837604522705\n",
            "\n",
            "Epoch - 6\n",
            "Epoch_Loss - 2.0863287448883057\n",
            "\n",
            "Epoch - 7\n",
            "Epoch_Loss - 1.623865008354187\n",
            "\n",
            "Epoch - 8\n",
            "Epoch_Loss - 3.311858892440796\n",
            "\n",
            "Epoch - 9\n",
            "Epoch_Loss - 1.4698431491851807\n",
            "\n",
            "Epoch - 10\n",
            "Epoch_Loss - 3.5562806129455566\n",
            "\n",
            "Epoch - 11\n",
            "Epoch_Loss - 1.1076253652572632\n",
            "\n",
            "Epoch - 12\n",
            "Epoch_Loss - 2.8540573120117188\n",
            "\n",
            "Epoch - 13\n",
            "Epoch_Loss - 0.5403867959976196\n",
            "\n",
            "Epoch - 14\n",
            "Epoch_Loss - 2.381434440612793\n",
            "\n",
            "Epoch - 15\n",
            "Epoch_Loss - 0.34060031175613403\n",
            "\n",
            "Epoch - 16\n",
            "Epoch_Loss - 1.7280659675598145\n",
            "\n",
            "Epoch - 17\n",
            "Epoch_Loss - 2.3184120655059814\n",
            "\n",
            "Epoch - 18\n",
            "Epoch_Loss - 2.0976369380950928\n",
            "\n",
            "Epoch - 19\n",
            "Epoch_Loss - 0.5977654457092285\n",
            "\n",
            "Epoch - 20\n",
            "Epoch_Loss - 1.1289169788360596\n",
            "\n",
            "Epoch - 21\n",
            "Epoch_Loss - 0.3504440188407898\n",
            "\n",
            "Epoch - 22\n",
            "Epoch_Loss - 1.139922857284546\n",
            "\n",
            "Epoch - 23\n",
            "Epoch_Loss - 1.3642802238464355\n",
            "\n",
            "Epoch - 24\n",
            "Epoch_Loss - 0.7164006233215332\n",
            "\n",
            "Epoch - 25\n",
            "Epoch_Loss - 0.5934737920761108\n",
            "\n",
            "Epoch - 26\n",
            "Epoch_Loss - 2.302639961242676\n",
            "\n",
            "Epoch - 27\n",
            "Epoch_Loss - 0.547495424747467\n",
            "\n",
            "Epoch - 28\n",
            "Epoch_Loss - 2.558650016784668\n",
            "\n",
            "Epoch - 29\n",
            "Epoch_Loss - 1.2662875652313232\n",
            "\n",
            "Epoch - 30\n",
            "Epoch_Loss - 0.013475568033754826\n",
            "\n",
            "66.97723525815267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2huZfH1B6Gx"
      },
      "source": [
        "outputs = []\n",
        "\n",
        "for src,trg in zip(hindi_test_sentence_list, english_test_sentence_list):\n",
        "    prediction = translate_sentence(model, src, device)\n",
        "    prediction = prediction[:-1]  # remove <eos> token\n",
        "    x = ' '.join([e for e in prediction])\n",
        "    outputs.append(x)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxwXNKENGc9o"
      },
      "source": [
        ""
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwnGPqaREaa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aebe85b-a78c-402c-f45c-76e3d50f4e75"
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C32flvd6BuBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483b3e74-2856-4ddf-ef63-9fa4b2ea1b7b"
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "nltk.download('wordnet')\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "\n",
        "references = english_test_sentence_list\n",
        "\n",
        "hypotheses = outputs\n",
        "\n",
        "total_num = len(references)\n",
        "total_bleu_scores = 0\n",
        "total_meteor_scores = 0\n",
        "for i in range(total_num):\n",
        "  total_bleu_scores+=sentence_bleu([references[i].split(\" \")], hypotheses[i].split(\" \"))\n",
        "  total_meteor_scores+=single_meteor_score(references[i], hypotheses[i])\n",
        "\n",
        "bleu_result = total_bleu_scores/total_num\n",
        "meteor_result = total_meteor_scores/total_num\n",
        "\n",
        "print(\"bleu score: \",bleu_result)\n",
        "print(\"meteor score: \",meteor_result)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "bleu score:  0.00047116058731513916\n",
            "meteor score:  0.10489189948948359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZNhI6Ijosmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad7f2af-5c90-4201-81e1-958f5daa9e23"
      },
      "source": [
        "print(epoch_loss / len(data_iter))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66.97723525815267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMmj7YiMul8b"
      },
      "source": [
        "testpath = '/content/drive/MyDrive/Colab Notebooks/hindistatements.csv'"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p3ga_RSulyF"
      },
      "source": [
        "finaldata = pd.read_csv(testpath,index_col=None)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3VS7TvdullV"
      },
      "source": [
        "finaldata = finaldata['hindi'].to_list()"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88dzuUTlHmd1"
      },
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/answer.txt', 'w') as f:\n",
        "    for sent in finaldata:\n",
        "        prediction = translate_sentence(model, sent, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "        x = ' '.join([e for e in prediction])\n",
        "        f.write(x+'\\n')"
      ],
      "execution_count": 76,
      "outputs": []
    }
  ]
}