{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n## **Seq2Seq Model with Attention**","metadata":{"id":"jUf1lKg4FwpL"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T12:19:02.976448Z","iopub.execute_input":"2021-08-07T12:19:02.976771Z","iopub.status.idle":"2021-08-07T12:19:03.082918Z","shell.execute_reply.started":"2021-08-07T12:19:02.976742Z","shell.execute_reply":"2021-08-07T12:19:03.081778Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/hindi2/hindistatements2.csv\n/kaggle/input/hineng/train.csv\n/kaggle/input/checkpoint/checkpoint-NMT-SD (1)\n/kaggle/input/hinmain/hindistatements.csv\n/kaggle/input/hindi3/hindistatements.csv\n/kaggle/input/hindi4/hindistatements4.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Read the Dataset**","metadata":{}},{"cell_type":"code","source":"filepath = '../input/hineng/train.csv'","metadata":{"id":"BXpa8cYz2-ih","execution":{"iopub.status.busy":"2021-08-07T12:19:03.084804Z","iopub.execute_input":"2021-08-07T12:19:03.085283Z","iopub.status.idle":"2021-08-07T12:19:03.091244Z","shell.execute_reply.started":"2021-08-07T12:19:03.085252Z","shell.execute_reply":"2021-08-07T12:19:03.090000Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import csv\n\nhindi_list=[]\nenglish_list=[]\n\nwith open('../input/hineng/train.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    for line in reader:\n        hindi_list.append(line['hindi'])\n        english_list.append(line['english'])","metadata":{"execution":{"iopub.status.busy":"2021-08-07T12:19:03.093058Z","iopub.execute_input":"2021-08-07T12:19:03.093532Z","iopub.status.idle":"2021-08-07T12:19:04.210294Z","shell.execute_reply.started":"2021-08-07T12:19:03.093490Z","shell.execute_reply":"2021-08-07T12:19:04.209426Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"hindi_list[:5], english_list[:5]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T12:19:04.211753Z","iopub.execute_input":"2021-08-07T12:19:04.219345Z","iopub.status.idle":"2021-08-07T12:19:04.229886Z","shell.execute_reply.started":"2021-08-07T12:19:04.219293Z","shell.execute_reply":"2021-08-07T12:19:04.228293Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(['एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध से वापसी ली, उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।',\n  'मैं उनके साथ कोई लेना देना नहीं है.',\n  '-हटाओ रिक.',\n  'क्योंकि यह एक खुशियों भरी फ़िल्म है.',\n  'The thought reaching the eyes...'],\n [\"In El Salvador, both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner's dilemma strategy.\",\n  'I have nothing to do with them.',\n  'Fuck them, Rick.',\n  \"Because it's a happy film.\",\n  'The thought reaching the eyes...'])"},"metadata":{}}]},{"cell_type":"markdown","source":"### **Train and Test Split**","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nindices_list = np.arange(len(hindi_list))\nnp.random.shuffle(indices_list)\ntrain_size = 0.8\ntrain_indices_list = indices_list[:int(len(indices_list)*train_size)]\ntest_indices_list = indices_list[int(len(indices_list)*train_size):]\n\nhindi_sentence_list = [hindi_list[i] for i in train_indices_list]\nenglish_sentence_list = [english_list[i] for i in train_indices_list]\n\nhindi_test_sentence_list = [hindi_list[i] for i in test_indices_list]\nenglish_test_sentence_list = [english_list[i] for i in test_indices_list]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T12:19:04.234265Z","iopub.execute_input":"2021-08-07T12:19:04.234714Z","iopub.status.idle":"2021-08-07T12:19:04.316356Z","shell.execute_reply.started":"2021-08-07T12:19:04.234673Z","shell.execute_reply":"2021-08-07T12:19:04.315625Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"len(hindi_sentence_list), len(english_sentence_list)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T12:19:04.319287Z","iopub.execute_input":"2021-08-07T12:19:04.319651Z","iopub.status.idle":"2021-08-07T12:19:04.325461Z","shell.execute_reply.started":"2021-08-07T12:19:04.319616Z","shell.execute_reply":"2021-08-07T12:19:04.324548Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(81857, 81857)"},"metadata":{}}]},{"cell_type":"code","source":"len(hindi_test_sentence_list), len(english_test_sentence_list)","metadata":{"id":"EXdg-U-ldeoo","outputId":"4e5d9f60-2f4c-4a9f-9f35-ed2473959a0d","execution":{"iopub.status.busy":"2021-08-07T12:19:04.327038Z","iopub.execute_input":"2021-08-07T12:19:04.327642Z","iopub.status.idle":"2021-08-07T12:19:04.336930Z","shell.execute_reply.started":"2021-08-07T12:19:04.327531Z","shell.execute_reply":"2021-08-07T12:19:04.335851Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(20465, 20465)"},"metadata":{}}]},{"cell_type":"markdown","source":"### **Install Indic NLP Library**","metadata":{"id":"ZhwpdMlDF8vv"}},{"cell_type":"code","source":"!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"","metadata":{"id":"ph3H3x0zDe2h","outputId":"e1814d6f-c070-4897-99d8-c54e2e6e0b26","execution":{"iopub.status.busy":"2021-08-07T12:19:04.338339Z","iopub.execute_input":"2021-08-07T12:19:04.338846Z","iopub.status.idle":"2021-08-07T12:19:07.229198Z","shell.execute_reply.started":"2021-08-07T12:19:04.338809Z","shell.execute_reply":"2021-08-07T12:19:07.228271Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Cloning into 'indic_nlp_library'...\nremote: Enumerating objects: 1325, done.\u001b[K\nremote: Counting objects: 100% (147/147), done.\u001b[K\nremote: Compressing objects: 100% (103/103), done.\u001b[K\nremote: Total 1325 (delta 84), reused 89 (delta 41), pack-reused 1178\u001b[K\nReceiving objects: 100% (1325/1325), 9.57 MiB | 9.57 MiB/s, done.\nResolving deltas: 100% (688/688), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git","metadata":{"id":"xM1dnvG6Dap1","outputId":"69fb66a9-e3c7-43ce-a568-3122e2b1ab07","execution":{"iopub.status.busy":"2021-08-07T12:19:07.232133Z","iopub.execute_input":"2021-08-07T12:19:07.232525Z","iopub.status.idle":"2021-08-07T12:19:16.353342Z","shell.execute_reply.started":"2021-08-07T12:19:07.232488Z","shell.execute_reply":"2021-08-07T12:19:16.352307Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Cloning into 'indic_nlp_resources'...\nremote: Enumerating objects: 133, done.\u001b[K\nremote: Counting objects: 100% (7/7), done.\u001b[K\nremote: Compressing objects: 100% (7/7), done.\u001b[K\nremote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\nReceiving objects: 100% (133/133), 149.77 MiB | 25.37 MiB/s, done.\nResolving deltas: 100% (51/51), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install Morfessor","metadata":{"id":"RRJ0Hd7nEc1H","outputId":"5ed9cf10-6d83-463d-d931-962058e0aa4a","execution":{"iopub.status.busy":"2021-08-07T12:19:16.356075Z","iopub.execute_input":"2021-08-07T12:19:16.356460Z","iopub.status.idle":"2021-08-07T12:19:24.115646Z","shell.execute_reply.started":"2021-08-07T12:19:16.356421Z","shell.execute_reply":"2021-08-07T12:19:24.114630Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting Morfessor\n  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\nInstalling collected packages: Morfessor\nSuccessfully installed Morfessor-2.0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"# The path to the local git repo for Indic NLP library\nINDIC_NLP_LIB_HOME=r\"./indic_nlp_library\"\n\n# The path to the local git repo for Indic NLP Resources\nINDIC_NLP_RESOURCES=\"./indic_nlp_resources\"","metadata":{"id":"BLa9zj92EglQ","execution":{"iopub.status.busy":"2021-08-07T12:19:24.118811Z","iopub.execute_input":"2021-08-07T12:19:24.119223Z","iopub.status.idle":"2021-08-07T12:19:24.130456Z","shell.execute_reply.started":"2021-08-07T12:19:24.119180Z","shell.execute_reply":"2021-08-07T12:19:24.129358Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))","metadata":{"id":"tIn-N_z0EnVC","execution":{"iopub.status.busy":"2021-08-07T12:19:24.133010Z","iopub.execute_input":"2021-08-07T12:19:24.133376Z","iopub.status.idle":"2021-08-07T12:19:24.142267Z","shell.execute_reply.started":"2021-08-07T12:19:24.133337Z","shell.execute_reply":"2021-08-07T12:19:24.140935Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from indicnlp import common\ncommon.set_resources_path(INDIC_NLP_RESOURCES)","metadata":{"id":"XBvcuzn_ErXf","execution":{"iopub.status.busy":"2021-08-07T12:19:24.143843Z","iopub.execute_input":"2021-08-07T12:19:24.144441Z","iopub.status.idle":"2021-08-07T12:19:24.157602Z","shell.execute_reply.started":"2021-08-07T12:19:24.144229Z","shell.execute_reply":"2021-08-07T12:19:24.155641Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from indicnlp import loader\nloader.load()","metadata":{"id":"43YadMNsEu1g","execution":{"iopub.status.busy":"2021-08-07T12:19:24.159614Z","iopub.execute_input":"2021-08-07T12:19:24.160485Z","iopub.status.idle":"2021-08-07T12:19:24.240200Z","shell.execute_reply.started":"2021-08-07T12:19:24.160441Z","shell.execute_reply":"2021-08-07T12:19:24.239073Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### **Tokenize Hindi sentences**","metadata":{"id":"sOKGcupGGPvD"}},{"cell_type":"code","source":"# Detokenize\n# Ref: https://colab.research.google.com/drive/1p3oGPcNdORw5_MDcufTDYWJhJt3XVPuC?usp=sharing#scrollTo=GU6E07Yw5zvl\n\nfrom indicnlp.tokenize import indic_detokenize  \n\nfor i in range(len(hindi_sentence_list)):\n  hindi_sentence_list[i] = indic_detokenize.trivial_detokenize(hindi_sentence_list[i],lang='hi')\n","metadata":{"id":"2CFni1UAG9y9","execution":{"iopub.status.busy":"2021-08-07T12:19:24.241907Z","iopub.execute_input":"2021-08-07T12:19:24.242281Z","iopub.status.idle":"2021-08-07T12:19:28.108281Z","shell.execute_reply.started":"2021-08-07T12:19:24.242245Z","shell.execute_reply":"2021-08-07T12:19:28.107507Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import string \nstring.punctuation","metadata":{"id":"ytzhhOAqyY-R","outputId":"ef72907a-0d20-4130-eec2-f59ae099097c","execution":{"iopub.status.busy":"2021-08-07T12:19:28.109570Z","iopub.execute_input":"2021-08-07T12:19:28.109917Z","iopub.status.idle":"2021-08-07T12:19:28.115972Z","shell.execute_reply.started":"2021-08-07T12:19:28.109882Z","shell.execute_reply":"2021-08-07T12:19:28.115167Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"},"metadata":{}}]},{"cell_type":"code","source":"#Ref for unicode chart: https://www.ssec.wisc.edu/~tomw/java/unicode.html#x0900 \n\nimport re\n\nfrom indicnlp.tokenize import indic_tokenize \n\nhindi_word_to_count={}\nhindi_word_to_index={'UNK':0, 'PAD':1, 'SOS':2, 'EOS':3}\nhindi_index_to_word={0:'UNK', 1:'PAD', 2:'SOS', 3:'EOS'}\ncount=4\nfor sent in hindi_sentence_list:\n  for t in indic_tokenize.trivial_tokenize(sent): \n    x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n    for elem in x:\n      hindi_word_to_count[elem] = hindi_word_to_count.get(elem,0)+1\n      if hindi_word_to_index.get(elem) is None and hindi_word_to_count.get(elem,0) >= 2:\n        hindi_word_to_index[elem] = count\n        hindi_index_to_word[count] = elem\n        count+=1\nprint(count)","metadata":{"id":"xQdSsjw8Nqwz","outputId":"4095ca63-8b38-4c5c-e19f-cc1e83c85586","execution":{"iopub.status.busy":"2021-08-07T12:19:28.117580Z","iopub.execute_input":"2021-08-07T12:19:28.118272Z","iopub.status.idle":"2021-08-07T12:19:32.053784Z","shell.execute_reply.started":"2021-08-07T12:19:28.118219Z","shell.execute_reply":"2021-08-07T12:19:32.052392Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"19062\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('./HindiWordToCount.txt','w') as f:\n  for k,v in hindi_word_to_count.items():\n    f.write(str(k)+\",\"+str(v)+\"\\n\")","metadata":{"id":"QdfXWlPzjzyM","execution":{"iopub.status.busy":"2021-08-07T12:19:32.055361Z","iopub.execute_input":"2021-08-07T12:19:32.055698Z","iopub.status.idle":"2021-08-07T12:19:32.102702Z","shell.execute_reply.started":"2021-08-07T12:19:32.055651Z","shell.execute_reply":"2021-08-07T12:19:32.102032Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print(len(hindi_word_to_count))","metadata":{"id":"Kohp2FyE_A1F","outputId":"b63939bc-6139-4d15-ed82-ec6583fbc450","execution":{"iopub.status.busy":"2021-08-07T12:19:32.104059Z","iopub.execute_input":"2021-08-07T12:19:32.104619Z","iopub.status.idle":"2021-08-07T12:19:32.109569Z","shell.execute_reply.started":"2021-08-07T12:19:32.104571Z","shell.execute_reply":"2021-08-07T12:19:32.108728Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"40102\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(hindi_word_to_index))","metadata":{"id":"1nIDutUfKbxa","outputId":"005d87c4-f3ef-4a02-d20e-9d048a5ecd39","execution":{"iopub.status.busy":"2021-08-07T12:19:32.111325Z","iopub.execute_input":"2021-08-07T12:19:32.111804Z","iopub.status.idle":"2021-08-07T12:19:32.118804Z","shell.execute_reply.started":"2021-08-07T12:19:32.111765Z","shell.execute_reply":"2021-08-07T12:19:32.117927Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"19062\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Tokenize English sentences**","metadata":{"id":"CSfh63mXqitj"}},{"cell_type":"code","source":"!python3 -m spacy download en","metadata":{"id":"HGb6O4X1oTyK","outputId":"cfa7cac8-92d4-4fa3-a4a7-8b92a001737b","execution":{"iopub.status.busy":"2021-08-07T12:19:32.120416Z","iopub.execute_input":"2021-08-07T12:19:32.120936Z","iopub.status.idle":"2021-08-07T12:19:45.157307Z","shell.execute_reply.started":"2021-08-07T12:19:32.120866Z","shell.execute_reply":"2021-08-07T12:19:45.156491Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Collecting en_core_web_sm==2.3.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n\u001b[K     |████████████████████████████████| 12.0 MB 13.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.56.2)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.5)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.6.0.post20210108)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\nRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.3)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('en_core_web_sm')\n\u001b[38;5;2m✔ Linking successful\u001b[0m\n/opt/conda/lib/python3.7/site-packages/en_core_web_sm -->\n/opt/conda/lib/python3.7/site-packages/spacy/data/en\nYou can now load the model via spacy.load('en')\n","output_type":"stream"}]},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"id":"Dn6zkR4-bctJ","execution":{"iopub.status.busy":"2021-08-07T12:19:45.159729Z","iopub.execute_input":"2021-08-07T12:19:45.159993Z","iopub.status.idle":"2021-08-07T12:19:47.412390Z","shell.execute_reply.started":"2021-08-07T12:19:45.159967Z","shell.execute_reply":"2021-08-07T12:19:47.411584Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"english_word_to_count={}\nenglish_word_to_index={'UNK':0, 'PAD':1, 'SOS':2, 'EOS':3}\nenglish_index_to_word={0:'UNK', 1:'PAD', 2:'SOS', 3:'EOS'}\ncount=4\n\nfor sent in english_sentence_list:\n  for token in nlp.tokenizer(sent.lower()):\n    temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token.text)\n    for elem in temp:\n      english_word_to_count[elem] = english_word_to_count.get(elem,0)+1\n      if english_word_to_index.get(elem) is None and english_word_to_count.get(elem,0) >= 2:\n        english_word_to_index[elem] = count\n        english_index_to_word[count] = elem\n        count+=1\nprint(count)","metadata":{"id":"MCGsCkw01Po3","outputId":"fbaeb645-892b-4270-8487-0190b209113c","execution":{"iopub.status.busy":"2021-08-07T12:19:47.413719Z","iopub.execute_input":"2021-08-07T12:19:47.414052Z","iopub.status.idle":"2021-08-07T12:19:56.221513Z","shell.execute_reply.started":"2021-08-07T12:19:47.414018Z","shell.execute_reply":"2021-08-07T12:19:56.220610Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"16692\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('./EnglishWordToCount.txt','w') as f:\n  for k,v in english_word_to_count.items():\n    f.write(str(k)+\",\"+str(v)+\"\\n\")","metadata":{"id":"BWeSR4fzth6M","execution":{"iopub.status.busy":"2021-08-07T12:19:56.222834Z","iopub.execute_input":"2021-08-07T12:19:56.223350Z","iopub.status.idle":"2021-08-07T12:19:56.254624Z","shell.execute_reply.started":"2021-08-07T12:19:56.223292Z","shell.execute_reply":"2021-08-07T12:19:56.253937Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### **Find Sentences Length and fix the maximum length and filter the sentences**\n","metadata":{"id":"WJvOrOpvefHa"}},{"cell_type":"markdown","source":"#### **First Let's Check for Hindi**","metadata":{"id":"n-ziiOx1ko9j"}},{"cell_type":"code","source":"sent_len_count={}\nfor sent in hindi_sentence_list:\n  sent_len=0\n  for t in indic_tokenize.trivial_tokenize(sent): \n    x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n    for elem in x:\n      sent_len+=1\n  sent_len_count[sent_len] = sent_len_count.get(sent_len,0)+1\n\nprint(sent_len_count)\n","metadata":{"id":"1s49M8J3ed4R","execution":{"iopub.status.busy":"2021-08-07T12:19:56.256002Z","iopub.execute_input":"2021-08-07T12:19:56.256470Z","iopub.status.idle":"2021-08-07T12:19:59.742273Z","shell.execute_reply.started":"2021-08-07T12:19:56.256431Z","shell.execute_reply":"2021-08-07T12:19:59.741301Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"{7: 6183, 10: 4169, 6: 6544, 29: 553, 14: 2383, 8: 5679, 26: 725, 11: 3635, 9: 5005, 5: 6357, 35: 349, 20: 1290, 13: 2784, 23: 954, 4: 5611, 18: 1517, 19: 1339, 16: 2021, 1: 319, 3: 3866, 2: 2730, 22: 1031, 12: 3220, 15: 2233, 27: 703, 34: 361, 24: 861, 28: 642, 17: 1571, 30: 525, 32: 403, 46: 119, 41: 174, 21: 1113, 31: 448, 47: 104, 42: 196, 38: 256, 39: 211, 33: 403, 40: 219, 25: 753, 65: 19, 60: 43, 37: 259, 36: 278, 44: 125, 43: 185, 77: 9, 52: 70, 63: 28, 64: 36, 53: 75, 58: 46, 50: 93, 56: 47, 62: 28, 61: 49, 49: 81, 88: 4, 45: 148, 54: 60, 48: 111, 83: 6, 72: 16, 69: 23, 96: 5, 57: 34, 51: 77, 92: 2, 68: 30, 59: 29, 89: 3, 66: 28, 105: 3, 75: 9, 70: 9, 55: 48, 71: 14, 104: 3, 73: 9, 84: 6, 101: 3, 80: 7, 86: 8, 67: 19, 82: 5, 93: 1, 79: 11, 78: 7, 74: 17, 109: 4, 81: 4, 90: 3, 99: 4, 118: 3, 87: 4, 95: 1, 127: 1, 122: 2, 107: 1, 179: 1, 141: 1, 76: 7, 85: 8, 98: 2, 103: 2, 180: 1, 97: 2, 223: 1, 111: 2, 144: 1, 116: 1, 100: 2, 94: 2, 131: 1, 106: 1, 0: 1, 91: 2, 113: 2, 172: 1, 133: 1, 138: 1, 596: 1, 165: 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"# sort the dictionary based on their counts\nimport operator\n\nsorted_counts = sorted(sent_len_count.items(), key=operator.itemgetter(1), reverse=True)\n\nprint(sorted_counts)\nindex=0\nfor pair in sorted_counts:\n  if pair[1]>300:\n    index+=1\n  else:\n    break\n\nmax_hindi_len=0\nfor pair in sorted_counts[:index]:\n  if pair[0]>max_hindi_len:\n    max_hindi_len=pair[0]\n\nprint(max_hindi_len)","metadata":{"id":"6d5Pti5FjYSV","execution":{"iopub.status.busy":"2021-08-07T12:19:59.743623Z","iopub.execute_input":"2021-08-07T12:19:59.744006Z","iopub.status.idle":"2021-08-07T12:19:59.751350Z","shell.execute_reply.started":"2021-08-07T12:19:59.743953Z","shell.execute_reply":"2021-08-07T12:19:59.750469Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"[(6, 6544), (5, 6357), (7, 6183), (8, 5679), (4, 5611), (9, 5005), (10, 4169), (3, 3866), (11, 3635), (12, 3220), (13, 2784), (2, 2730), (14, 2383), (15, 2233), (16, 2021), (17, 1571), (18, 1517), (19, 1339), (20, 1290), (21, 1113), (22, 1031), (23, 954), (24, 861), (25, 753), (26, 725), (27, 703), (28, 642), (29, 553), (30, 525), (31, 448), (32, 403), (33, 403), (34, 361), (35, 349), (1, 319), (36, 278), (37, 259), (38, 256), (40, 219), (39, 211), (42, 196), (43, 185), (41, 174), (45, 148), (44, 125), (46, 119), (48, 111), (47, 104), (50, 93), (49, 81), (51, 77), (53, 75), (52, 70), (54, 60), (61, 49), (55, 48), (56, 47), (58, 46), (60, 43), (64, 36), (57, 34), (68, 30), (59, 29), (63, 28), (62, 28), (66, 28), (69, 23), (65, 19), (67, 19), (74, 17), (72, 16), (71, 14), (79, 11), (77, 9), (75, 9), (70, 9), (73, 9), (86, 8), (85, 8), (80, 7), (78, 7), (76, 7), (83, 6), (84, 6), (96, 5), (82, 5), (88, 4), (109, 4), (81, 4), (99, 4), (87, 4), (89, 3), (105, 3), (104, 3), (101, 3), (90, 3), (118, 3), (92, 2), (122, 2), (98, 2), (103, 2), (97, 2), (111, 2), (100, 2), (94, 2), (91, 2), (113, 2), (93, 1), (95, 1), (127, 1), (107, 1), (179, 1), (141, 1), (180, 1), (223, 1), (144, 1), (116, 1), (131, 1), (106, 1), (0, 1), (172, 1), (133, 1), (138, 1), (596, 1), (165, 1)]\n35\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **Now Let's check for English**","metadata":{"id":"NZQe8m3kmEyg"}},{"cell_type":"code","source":"english_sent_len_count={}\nfor sent in english_sentence_list:\n  sent_len=0\n  for token in nlp.tokenizer(sent.lower()): \n    temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token.text)\n    for elem in temp:\n      sent_len+=1\n  english_sent_len_count[sent_len] = english_sent_len_count.get(sent_len,0)+1\n\nprint(english_sent_len_count)","metadata":{"id":"h9LgNSHg21Fe","outputId":"a7383236-2454-485f-a266-29e737bef48e","execution":{"iopub.status.busy":"2021-08-07T12:19:59.752596Z","iopub.execute_input":"2021-08-07T12:19:59.753107Z","iopub.status.idle":"2021-08-07T12:20:07.277043Z","shell.execute_reply.started":"2021-08-07T12:19:59.753068Z","shell.execute_reply":"2021-08-07T12:20:07.275966Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"{10: 4263, 4: 6245, 8: 5737, 6: 7312, 25: 689, 5: 7107, 13: 2722, 9: 4820, 21: 1033, 11: 3484, 12: 3181, 7: 6622, 3: 3935, 26: 681, 30: 464, 20: 1168, 23: 860, 18: 1397, 16: 1797, 17: 1560, 15: 2004, 19: 1318, 35: 261, 2: 2925, 22: 944, 31: 440, 42: 149, 14: 2345, 38: 226, 34: 314, 48: 100, 27: 599, 39: 189, 41: 169, 32: 401, 36: 256, 24: 766, 33: 332, 29: 493, 28: 554, 52: 59, 56: 45, 46: 100, 70: 11, 44: 144, 1: 94, 50: 74, 58: 31, 55: 45, 47: 99, 37: 215, 43: 141, 53: 43, 49: 75, 54: 47, 40: 189, 61: 23, 67: 14, 69: 13, 78: 5, 45: 109, 63: 13, 60: 29, 86: 4, 91: 2, 57: 43, 51: 66, 68: 12, 59: 34, 83: 4, 62: 24, 104: 1, 71: 16, 94: 2, 64: 23, 72: 8, 81: 3, 65: 13, 66: 13, 114: 1, 76: 6, 77: 7, 82: 3, 80: 8, 85: 4, 74: 6, 89: 5, 100: 1, 73: 9, 93: 3, 99: 2, 108: 1, 112: 1, 97: 1, 79: 3, 92: 3, 171: 1, 143: 1, 106: 3, 84: 5, 282: 1, 164: 1, 88: 5, 95: 2, 261: 1, 87: 2, 116: 1, 75: 7, 110: 1, 107: 1, 90: 5, 121: 1, 169: 1, 123: 1, 446: 1, 103: 2, 96: 1, 157: 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"# sort the dictionary based on their counts\nimport operator\n\nenglish_sorted_counts = sorted(english_sent_len_count.items(), key=operator.itemgetter(1), reverse=True)\n\nindex=0\nfor pair in english_sorted_counts:\n  if pair[1]>300:\n    index+=1\n  else:\n    break\n\nprint(index)\nmax_english_len=0\nfor pair in english_sorted_counts[:index]:\n  if pair[0]>max_english_len:\n    max_english_len=pair[0]\n\nprint(max_english_len)","metadata":{"id":"gEOBhAxDc8gs","execution":{"iopub.status.busy":"2021-08-07T12:20:07.278618Z","iopub.execute_input":"2021-08-07T12:20:07.279002Z","iopub.status.idle":"2021-08-07T12:20:07.288531Z","shell.execute_reply.started":"2021-08-07T12:20:07.278967Z","shell.execute_reply":"2021-08-07T12:20:07.286597Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"33\n34\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **Get the maximum Length**","metadata":{}},{"cell_type":"code","source":"max_len = max(max_hindi_len, max_english_len)\nmax_len","metadata":{"id":"DCGccsRC-QVb","execution":{"iopub.status.busy":"2021-08-07T12:20:07.290757Z","iopub.execute_input":"2021-08-07T12:20:07.291363Z","iopub.status.idle":"2021-08-07T12:20:07.302641Z","shell.execute_reply.started":"2021-08-07T12:20:07.291292Z","shell.execute_reply":"2021-08-07T12:20:07.301498Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"35"},"metadata":{}}]},{"cell_type":"markdown","source":"### **Filter out sentences with length less than or equal to maximum length**","metadata":{"id":"HAbUhu62u18-"}},{"cell_type":"code","source":"english_filtered_sent_list=[]\nhindi_filtered_sent_list=[]\nfiltered_sent_pair_list=[]\nfor hin_sent, eng_sent in zip(hindi_sentence_list, english_sentence_list):\n  hin_sent_len=0\n  for t in indic_tokenize.trivial_tokenize(hin_sent): \n    x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n    for elem in x:\n      hin_sent_len+=1\n\n  eng_sent_len=0\n  for token in nlp.tokenizer(eng_sent.lower()): \n    temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token.text)\n    for elem in temp:\n      eng_sent_len+=1\n\n  if hin_sent_len>=1 and hin_sent_len<=max_len and eng_sent_len>=1 and eng_sent_len<=max_len:\n    english_filtered_sent_list.append(eng_sent)\n    hindi_filtered_sent_list.append(hin_sent)\n    filtered_sent_pair_list.append([hin_sent, eng_sent])\n\nprint(len(english_filtered_sent_list))\nprint(len(hindi_filtered_sent_list))\nprint(len(filtered_sent_pair_list))\nprint(filtered_sent_pair_list[:5])","metadata":{"id":"uP5Vr6pfu1AV","execution":{"iopub.status.busy":"2021-08-07T12:20:07.304415Z","iopub.execute_input":"2021-08-07T12:20:07.304910Z","iopub.status.idle":"2021-08-07T12:20:18.104757Z","shell.execute_reply.started":"2021-08-07T12:20:07.304746Z","shell.execute_reply":"2021-08-07T12:20:18.104021Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"77854\n77854\n77854\n[['बोलो, Rom, थूको मत.', \"Say it, don't spray it, Rom.\"], ['मैं भी उस समय टूट गया।', 'This was mine.'], ['मुझे आपकी ज़रूरत है टोटेम नष्ट करने के लिए।', 'I need you to destroy the totem.'], ['मैं उससे मिलना चाहती हूँ.', 'I want to meet her.'], ['मुझे लगता है की मैं अपने ग्राहकों की जरूरतों को पूरा कर सकती हूँ भविष्य की पीढ़ियों की एक हरी कल में रहने की क्षमता से समझौता किए बगैर', 'I feel that I can meet the needs of my customers without compromising the ability of future generations to live in a greener tomorrow.']]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Store the filtered hindi sentences in a file\n\nwith open('./FilteredSentencesPair.txt','w') as f:\n  for line in filtered_sent_pair_list:\n    f.write(line[0]+','+line[1]+'\\n')","metadata":{"id":"pymFPol0wcqR","execution":{"iopub.status.busy":"2021-08-07T12:20:18.107176Z","iopub.execute_input":"2021-08-07T12:20:18.107536Z","iopub.status.idle":"2021-08-07T12:20:18.220123Z","shell.execute_reply.started":"2021-08-07T12:20:18.107502Z","shell.execute_reply":"2021-08-07T12:20:18.219388Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### **Form List of list of indexes from the filtered sentences**","metadata":{"id":"DciCZiuq3P8O"}},{"cell_type":"code","source":"# Create tensor array for each hindi sentence \nimport torch\nhindi_list_indices = torch.tensor([[1]*(max_len+2)]*len(filtered_sent_pair_list), dtype=torch.long)\n\ni=0\nfor sent in hindi_filtered_sent_list:\n  hindi_list_indices[i][0]=2  #SOS\n  j=1\n  for t in indic_tokenize.trivial_tokenize(sent): \n    x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n    for elem in x:\n      if hindi_word_to_index.get(elem) is None:\n        continue\n      else:\n        hindi_list_indices[i][j] = hindi_word_to_index.get(elem)\n      j+=1\n  hindi_list_indices[i][j]=3  #EOS\n  j+=1\n  while j<=(max_len+1):\n    hindi_list_indices[i][j] = 1  #PAD\n    j+=1\n\n  i+=1\n\n# Create tensor array for each hindi test sentence\nhindi_test_list_indices = torch.tensor([[1]*(max_len+2)]*len(hindi_test_sentence_list), dtype=torch.long)\n\ni=0\nfor sent in hindi_test_sentence_list:\n  hindi_test_list_indices[i][0]=2  #SOS\n  j=1\n  for t in indic_tokenize.trivial_tokenize(sent): \n    x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n    for elem in x:\n      if j>(max_len):\n        break\n      if hindi_word_to_index.get(elem) is None:\n        continue\n        #hindi_test_list_indices[i][j] = 0  # UNK\n      else:\n        hindi_test_list_indices[i][j] = hindi_word_to_index.get(elem)\n      j+=1\n  hindi_test_list_indices[i][j]=3  #EOS\n  j+=1\n  while j<=(max_len+1):\n    hindi_test_list_indices[i][j] = 1  #PAD\n    j+=1\n\n  i+=1\n\n\n# Create tensor array for each english sentence\n\nenglish_list_indices = torch.tensor([[1]*(max_len+2)]*len(filtered_sent_pair_list), dtype=torch.long)\n\ni=0\nfor sent in english_filtered_sent_list:\n  english_list_indices[i][0]=2  #SOS\n  j=1\n  for token in nlp.tokenizer(sent.lower()):\n    temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token.text)\n    for elem in temp:\n      if english_word_to_index.get(elem) is None:\n        continue\n        #english_list_indices[i][j] = 0  # UNK\n      else:\n        english_list_indices[i][j] = english_word_to_index.get(elem)\n      j+=1\n  english_list_indices[i][j]=3  #EOS\n  j+=1\n  while j<=(max_len+1):\n    english_list_indices[i][j] = 1  #PAD\n    j+=1\n  \n  i+=1\n\n\n# Create tensor array for each english test sentence\nenglish_test_list_indices = torch.tensor([[1]*(max_len+2)]*len(english_test_sentence_list), dtype=torch.long)\n\ni=0\nfor sent in english_test_sentence_list:\n  english_test_list_indices[i][0]=2  #SOS\n  j=1\n  for token in nlp.tokenizer(sent.lower()):\n    temp = re.findall('[A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', token.text)\n    for elem in temp:\n      if j>(max_len):\n        break\n      if english_word_to_index.get(elem) is None:\n        continue\n        #english_test_list_indices[i][j] = 0  # UNK\n      else:\n        english_test_list_indices[i][j] = english_word_to_index.get(elem)\n      j+=1\n  english_test_list_indices[i][j]=3  #EOS\n  j+=1\n  while j<=(max_len+1):\n    english_test_list_indices[i][j] = 1  #PAD\n    j+=1\n  \n  i+=1\n\nprint(hindi_list_indices[:5])\nprint(english_list_indices[:5])","metadata":{"id":"56Dc9KvM3NOC","execution":{"iopub.status.busy":"2021-08-07T12:20:18.226133Z","iopub.execute_input":"2021-08-07T12:20:18.226425Z","iopub.status.idle":"2021-08-07T12:21:40.333541Z","shell.execute_reply.started":"2021-08-07T12:20:18.226398Z","shell.execute_reply":"2021-08-07T12:21:40.332624Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"tensor([[   2, 1378,    4, 3621,    4,   14,    7,    3,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1],\n        [   2,    6,   48,   32,   77, 1104,   12,    5,    3,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1],\n        [   2,    8,   44,  313,    9, 5735,  202,   37,   19,   38,    5,    3,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1],\n        [   2,    6,   27,  231,  466,   11,    7,    3,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1],\n        [   2,    8,   39,    9,   10,    6,   51,  800,   10, 3291,   36,  178,\n           17,   89,   11,  786,   10, 2340,   10,   21, 5682,  260,   22,  115,\n           10,  897,   16,  717,  710, 8651,    3,    1,    1,    1,    1,    1,\n            1]])\ntensor([[   2,   91,    4,    5,   12,   13, 7509,    4,    5, 3334,    6,    3,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1],\n        [   2,   36,   18,  477,    6,    3,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1],\n        [   2,    7,   40,   15,    8,  165,   10, 5247,    6,    3,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1],\n        [   2,    7,   80,    8,    9,   35,    6,    3,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1],\n        [   2,    7,   54,   24,    7,   30,    9,   10,  140,   11,   17, 1256,\n          177,   10,  786,   11,  677, 2067,    8,  352,   20,   19, 8718,  220,\n            6,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1]])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(hindi_list_indices.shape)\nprint(english_list_indices.shape)","metadata":{"id":"leztTC8ziQmQ","execution":{"iopub.status.busy":"2021-08-07T12:21:40.335413Z","iopub.execute_input":"2021-08-07T12:21:40.335766Z","iopub.status.idle":"2021-08-07T12:21:40.340443Z","shell.execute_reply.started":"2021-08-07T12:21:40.335728Z","shell.execute_reply":"2021-08-07T12:21:40.339624Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"torch.Size([77854, 37])\ntorch.Size([77854, 37])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **Use GPU**","metadata":{}},{"cell_type":"code","source":"import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" \nprint(device)","metadata":{"id":"2RM5t4w5-kkF","execution":{"iopub.status.busy":"2021-08-07T12:21:40.342128Z","iopub.execute_input":"2021-08-07T12:21:40.342619Z","iopub.status.idle":"2021-08-07T12:21:40.389603Z","shell.execute_reply.started":"2021-08-07T12:21:40.342577Z","shell.execute_reply":"2021-08-07T12:21:40.388700Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **References:** \n### http://ethen8181.github.io/machine-learning/deep_learning/seq2seq/2_torch_seq2seq_attention.html \n### https://arxiv.org/abs/1409.0473","metadata":{"id":"_9381KgYUBuu"}},{"cell_type":"markdown","source":"## **Sequence to Sequence Model with Attention**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"id":"D0OWOYmqCpwm","execution":{"iopub.status.busy":"2021-08-07T12:21:40.391032Z","iopub.execute_input":"2021-08-07T12:21:40.391618Z","iopub.status.idle":"2021-08-07T12:21:40.397909Z","shell.execute_reply.started":"2021-08-07T12:21:40.391576Z","shell.execute_reply":"2021-08-07T12:21:40.397109Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Bidiretional GRU Encoder\nclass Encoder(nn.Module):\n\n  def __init__(self, input_size, embedding_size, hidden_dimension, num_layers, dropout):  \n    super().__init__()\n    \n    self.input_size=input_size\n    self.dropout=dropout\n    self.hidden_dimension = hidden_dimension\n    self.num_layers = num_layers\n\n    self.embedding = nn.Embedding(input_size, embedding_size)\n\n    self.rnn = nn.GRU(embedding_size, hidden_dimension,num_layers, dropout=dropout, bidirectional=True)\n\n    self.final_layer = nn.Linear(hidden_dimension * 2, hidden_dimension)\n\n  def forward(self, word_inputs):\n \n    # First embed the input words and then pass it to the encoder\n    embedded = self.embedding(word_inputs)\n\n    outputs, hidden = self.rnn(embedded)\n    \n    # Since the deoder is unidirectional, concatenate the hidden states\n    x = torch.cat((hidden[::2], hidden[1::2]), dim=2)\n\n    # The concatenated hidden vectors are then passed on to the layer with hidden dimension as the output dimension and tanh function\n    # as the activation function\n    hidden = torch.tanh(self.final_layer(x))\n\n    return outputs, hidden","metadata":{"id":"aiDTgw5014t_","execution":{"iopub.status.busy":"2021-08-07T12:21:40.399406Z","iopub.execute_input":"2021-08-07T12:21:40.400027Z","iopub.status.idle":"2021-08-07T12:21:40.409880Z","shell.execute_reply.started":"2021-08-07T12:21:40.399989Z","shell.execute_reply":"2021-08-07T12:21:40.409125Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Attention uses the previous state of the decoder and final layer hidden states\nclass Attention(nn.Module):\n\n    def __init__(self, hidden_dimension):\n        super().__init__()\n        self.hidden_dimension = hidden_dimension\n\n        self.final_layer1 = nn.Linear(hidden_dimension * 2 + hidden_dimension, hidden_dimension)\n        self.final_layer2 = nn.Linear(hidden_dimension, 1, bias=False)\n\n    def forward(self, encoder_outputs, hidden):\n        # encoder_ouptuts shape: [sequence_length, batch_size, hidden_dimension*2]\n        sequence_length = encoder_outputs.shape[0]\n        batch_size = encoder_outputs.shape[1]\n        \n        # hidden shape: [batch_size, hidden_dimension]\n        hidden = hidden.unsqueeze(1).repeat(1, sequence_length, 1)\n        #hidden shape: [batch_size, sequence_length, hidden_dimension]\n\n        outputs = encoder_outputs.permute(1, 0, 2)\n        # ouputs shape: [batch_size, sequence_length, hidden_dimension*2]\n\n        x = torch.cat((hidden, outputs), dim=2)\n\n        energy = torch.tanh(self.final_layer1(x))\n\n        attention = self.final_layer2(energy).squeeze(dim=2)        \n        attention_weight = torch.softmax(attention, dim=1)\n\n        return attention_weight","metadata":{"id":"H-kgdIYkJo3G","execution":{"iopub.status.busy":"2021-08-07T12:21:40.411234Z","iopub.execute_input":"2021-08-07T12:21:40.411797Z","iopub.status.idle":"2021-08-07T12:21:40.421814Z","shell.execute_reply.started":"2021-08-07T12:21:40.411759Z","shell.execute_reply":"2021-08-07T12:21:40.420990Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n  def __init__(self, output_size, embedding_size, hidden_dimension, num_layers, dropout, attention):\n    super().__init__()\n    \n    self.embedding_size=embedding_size\n    self.output_size = output_size\n    self.hidden_dimension = hidden_dimension\n    self.num_layers = num_layers\n    self.dropout = dropout\n    self.attention = attention\n\n    self.embedding = nn.Embedding(output_size, embedding_size)\n    \n    self.rnn = nn.GRU(hidden_dimension * 2 + embedding_size, hidden_dimension, num_layers, dropout = dropout)\n    \n    self.linear = nn.Linear(hidden_dimension, output_size)\n    \n  def forward(self, input, encoder_states, hidden):\n\n    attention = self.attention(encoder_states, hidden[-1]).unsqueeze(1)\n\n    outputs = encoder_states.permute(1, 0, 2)\n\n    context = torch.bmm(attention, outputs).permute(1, 0, 2)\n\n    embedded = self.embedding(input.unsqueeze(0))\n    x = torch.cat((embedded, context), dim=2)\n\n    outputs, hidden = self.rnn(x, hidden)\n    prediction = self.linear(outputs.squeeze(0))\n    return prediction, hidden.squeeze(0)","metadata":{"id":"UdKil5S03aZa","execution":{"iopub.status.busy":"2021-08-07T12:21:40.423378Z","iopub.execute_input":"2021-08-07T12:21:40.423766Z","iopub.status.idle":"2021-08-07T12:21:40.435676Z","shell.execute_reply.started":"2021-08-07T12:21:40.423730Z","shell.execute_reply":"2021-08-07T12:21:40.434830Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n  def __init__(self, encoder, decoder, device):\n    super().__init__()\n    \n    self.encoder = encoder\n    self.decoder = decoder\n    self.device = device\n      \n  def forward(self, source, target, teacher_force =0.75):\n\n    batch_size = target.shape[1]\n    sequence_length = target.shape[0]\n\n    target_dict_size = self.decoder.output_size\n    \n    pred_output = torch.zeros(sequence_length, batch_size, target_dict_size).to(self.device)\n    \n    encoder_states, hidden = self.encoder(source)\n    \n    input = target[0]\n    \n    for i in range(1, sequence_length):\n\n      output, hidden = self.decoder(input, encoder_states, hidden)\n\n      pred_output[i] = output\n    \n      best_pred = output.argmax(1) \n\n      if random.random() < teacher_force:\n        input = target[i]\n\n      else:\n        input = best_pred\n    \n    return pred_output","metadata":{"id":"QaGtBf8Xv0Xz","execution":{"iopub.status.busy":"2021-08-07T12:21:40.437748Z","iopub.execute_input":"2021-08-07T12:21:40.438031Z","iopub.status.idle":"2021-08-07T12:21:40.448643Z","shell.execute_reply.started":"2021-08-07T12:21:40.437997Z","shell.execute_reply":"2021-08-07T12:21:40.446915Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"### **Set Model Parameters and define Model**","metadata":{"id":"wvECVNV77wFQ"}},{"cell_type":"code","source":"input_size = len(hindi_word_to_index)\noutput_size = len(english_word_to_index)\nembedding_size = 128\nhidden_dimension = 256\nnum_layers = 2\ndropout = 0.5\nbatch_size=32\n\nattention = Attention(hidden_dimension)\nenc = Encoder(input_size, embedding_size, hidden_dimension, num_layers, dropout).to(device)\ndec = Decoder(output_size, embedding_size, hidden_dimension, num_layers, dropout, attention).to(device)\n\nmodel = Seq2Seq(enc, dec, device).to(device)\n\nprint(enc)\nprint(dec)\nprint(model)","metadata":{"id":"EqoT3XxmwqZu","execution":{"iopub.status.busy":"2021-08-07T12:21:40.449913Z","iopub.execute_input":"2021-08-07T12:21:40.450290Z","iopub.status.idle":"2021-08-07T12:21:44.928259Z","shell.execute_reply.started":"2021-08-07T12:21:40.450238Z","shell.execute_reply":"2021-08-07T12:21:44.927324Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Encoder(\n  (embedding): Embedding(19062, 128)\n  (rnn): GRU(128, 256, num_layers=2, dropout=0.5, bidirectional=True)\n  (final_layer): Linear(in_features=512, out_features=256, bias=True)\n)\nDecoder(\n  (attention): Attention(\n    (final_layer1): Linear(in_features=768, out_features=256, bias=True)\n    (final_layer2): Linear(in_features=256, out_features=1, bias=False)\n  )\n  (embedding): Embedding(16692, 128)\n  (rnn): GRU(640, 256, num_layers=2, dropout=0.5)\n  (linear): Linear(in_features=256, out_features=16692, bias=True)\n)\nSeq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(19062, 128)\n    (rnn): GRU(128, 256, num_layers=2, dropout=0.5, bidirectional=True)\n    (final_layer): Linear(in_features=512, out_features=256, bias=True)\n  )\n  (decoder): Decoder(\n    (attention): Attention(\n      (final_layer1): Linear(in_features=768, out_features=256, bias=True)\n      (final_layer2): Linear(in_features=256, out_features=1, bias=False)\n    )\n    (embedding): Embedding(16692, 128)\n    (rnn): GRU(640, 256, num_layers=2, dropout=0.5)\n    (linear): Linear(in_features=256, out_features=16692, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Test Encoder**","metadata":{"id":"TGMaINRf5cpi"}},{"cell_type":"code","source":"word_input = torch.zeros((7, 4), dtype=torch.long, device=device)  # here 7 is seq length and 4 is batch size\nprint(word_input.shape)\nout, enc_hid = enc(word_input)  # encode this word_input\n\nprint(enc_hid)\n\nprint(enc_hid.shape) # [num_layers, seq_length, hidden_units]","metadata":{"id":"JXy9rWYl7ugV","execution":{"iopub.status.busy":"2021-08-07T12:21:44.929602Z","iopub.execute_input":"2021-08-07T12:21:44.930126Z","iopub.status.idle":"2021-08-07T12:21:45.019925Z","shell.execute_reply.started":"2021-08-07T12:21:44.930085Z","shell.execute_reply":"2021-08-07T12:21:45.018943Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"torch.Size([7, 4])\ntensor([[[-0.0185,  0.0578,  0.3347,  ...,  0.1850, -0.1256, -0.4478],\n         [-0.0185,  0.0578,  0.3347,  ...,  0.1850, -0.1256, -0.4478],\n         [-0.0185,  0.0578,  0.3347,  ...,  0.1850, -0.1256, -0.4478],\n         [-0.0185,  0.0578,  0.3347,  ...,  0.1850, -0.1256, -0.4478]],\n\n        [[ 0.2067, -0.2617, -0.1299,  ..., -0.1121, -0.1026,  0.1087],\n         [ 0.1723, -0.1295,  0.0345,  ..., -0.0705, -0.0513,  0.2374],\n         [ 0.1768, -0.2075,  0.0048,  ..., -0.1104, -0.1819,  0.0840],\n         [ 0.2841, -0.1245,  0.0143,  ..., -0.1391, -0.1000,  0.0962]]],\n       device='cuda:0', grad_fn=<TanhBackward>)\ntorch.Size([2, 4, 256])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Test Decoder**","metadata":{"id":"Y2eHI-7VB1tY"}},{"cell_type":"code","source":"for i in range(7):\n    input = word_input[i]\n    pred, dec_hid= dec(input, out, enc_hid)\n    print(dec_hid.shape, pred)","metadata":{"id":"z5RUUFp-_9WA","execution":{"iopub.status.busy":"2021-08-07T12:21:45.021163Z","iopub.execute_input":"2021-08-07T12:21:45.021692Z","iopub.status.idle":"2021-08-07T12:21:45.058250Z","shell.execute_reply.started":"2021-08-07T12:21:45.021644Z","shell.execute_reply":"2021-08-07T12:21:45.056597Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"torch.Size([2, 4, 256]) tensor([[-0.1202,  0.0229, -0.0214,  ..., -0.0571, -0.0622,  0.0572],\n        [-0.0417, -0.0058,  0.0288,  ...,  0.0336, -0.0163,  0.0434],\n        [-0.1417,  0.0561, -0.0353,  ...,  0.0146, -0.1423,  0.0089],\n        [-0.1656,  0.0200,  0.0359,  ...,  0.0011, -0.0821,  0.0564]],\n       device='cuda:0', grad_fn=<AddmmBackward>)\ntorch.Size([2, 4, 256]) tensor([[-0.0804,  0.0034,  0.0134,  ...,  0.0020, -0.0403,  0.0719],\n        [-0.1363,  0.0450,  0.0258,  ...,  0.0703, -0.0414,  0.0829],\n        [-0.1405, -0.0046,  0.0147,  ..., -0.0541, -0.0128,  0.0623],\n        [-0.1217, -0.0485, -0.0050,  ...,  0.0094, -0.0972,  0.0960]],\n       device='cuda:0', grad_fn=<AddmmBackward>)\ntorch.Size([2, 4, 256]) tensor([[-0.1563, -0.0049, -0.0652,  ..., -0.0679, -0.0857,  0.0564],\n        [-0.1242,  0.0310,  0.0437,  ...,  0.0419, -0.0176, -0.0050],\n        [-0.1432,  0.0190, -0.0138,  ...,  0.0309, -0.0728,  0.0726],\n        [-0.1093, -0.0363, -0.0146,  ..., -0.0076, -0.0647,  0.0302]],\n       device='cuda:0', grad_fn=<AddmmBackward>)\ntorch.Size([2, 4, 256]) tensor([[-0.0490, -0.0560, -0.0176,  ..., -0.0374, -0.0393,  0.0060],\n        [-0.0627, -0.0351,  0.0348,  ..., -0.0062, -0.0124,  0.1021],\n        [-0.0939, -0.0105, -0.0575,  ...,  0.0517, -0.0202,  0.0554],\n        [-0.1084, -0.0371, -0.0458,  ..., -0.0040, -0.1054,  0.0752]],\n       device='cuda:0', grad_fn=<AddmmBackward>)\ntorch.Size([2, 4, 256]) tensor([[-0.0939,  0.0093, -0.0010,  ..., -0.0417, -0.0051,  0.0396],\n        [-0.0938, -0.0044, -0.0211,  ...,  0.0135, -0.0542,  0.0781],\n        [-0.0314, -0.0499, -0.0568,  ...,  0.0431, -0.0210, -0.0237],\n        [-0.1115, -0.0731, -0.0304,  ...,  0.0035, -0.0523,  0.0660]],\n       device='cuda:0', grad_fn=<AddmmBackward>)\ntorch.Size([2, 4, 256]) tensor([[-0.1109, -0.0217, -0.0470,  ..., -0.0086, -0.0620,  0.0708],\n        [-0.1072, -0.0032,  0.0132,  ...,  0.0254, -0.0753,  0.0697],\n        [-0.1200,  0.0173,  0.0261,  ..., -0.0224, -0.0442, -0.0246],\n        [-0.1326,  0.0414, -0.0511,  ...,  0.0199, -0.0951,  0.0752]],\n       device='cuda:0', grad_fn=<AddmmBackward>)\ntorch.Size([2, 4, 256]) tensor([[-0.0163, -0.0552, -0.0022,  ...,  0.0088, -0.0195,  0.0351],\n        [-0.1111, -0.0005,  0.0312,  ...,  0.0082, -0.0806,  0.1029],\n        [-0.0853,  0.0265, -0.0211,  ...,  0.0398, -0.0668, -0.0141],\n        [-0.1301, -0.0176, -0.0329,  ...,  0.0664, -0.0747,  0.0137]],\n       device='cuda:0', grad_fn=<AddmmBackward>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Initialize Weights**","metadata":{"id":"8oLUvmCQPsGD"}},{"cell_type":"code","source":"def init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n        \nmodel.apply(init_weights)","metadata":{"id":"MjLV2gluyIcI","execution":{"iopub.status.busy":"2021-08-07T12:21:45.059679Z","iopub.execute_input":"2021-08-07T12:21:45.060037Z","iopub.status.idle":"2021-08-07T12:21:45.071307Z","shell.execute_reply.started":"2021-08-07T12:21:45.060000Z","shell.execute_reply":"2021-08-07T12:21:45.070147Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(19062, 128)\n    (rnn): GRU(128, 256, num_layers=2, dropout=0.5, bidirectional=True)\n    (final_layer): Linear(in_features=512, out_features=256, bias=True)\n  )\n  (decoder): Decoder(\n    (attention): Attention(\n      (final_layer1): Linear(in_features=768, out_features=256, bias=True)\n      (final_layer2): Linear(in_features=256, out_features=1, bias=False)\n    )\n    (embedding): Embedding(16692, 128)\n    (rnn): GRU(640, 256, num_layers=2, dropout=0.5)\n    (linear): Linear(in_features=256, out_features=16692, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### **Optimizer**","metadata":{"id":"9ieuR_dK0Jwn"}},{"cell_type":"code","source":"from torch import optim\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"id":"3Htf9iuFfAcv","execution":{"iopub.status.busy":"2021-08-07T12:21:45.072693Z","iopub.execute_input":"2021-08-07T12:21:45.073030Z","iopub.status.idle":"2021-08-07T12:21:45.078130Z","shell.execute_reply.started":"2021-08-07T12:21:45.072995Z","shell.execute_reply":"2021-08-07T12:21:45.077087Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"### **Loss**","metadata":{"id":"xJEV_8v24MZY"}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index = 1)","metadata":{"id":"MepBgJv64Lrt","execution":{"iopub.status.busy":"2021-08-07T12:21:45.079768Z","iopub.execute_input":"2021-08-07T12:21:45.080198Z","iopub.status.idle":"2021-08-07T12:21:45.088152Z","shell.execute_reply.started":"2021-08-07T12:21:45.080162Z","shell.execute_reply":"2021-08-07T12:21:45.087225Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### **Create Batches**","metadata":{}},{"cell_type":"code","source":"from torch.utils import data\ndef load_array(data_arrays, batch_size, is_train=True):\n    dataset = data.TensorDataset(*data_arrays)\n    return data.DataLoader(dataset, batch_size, shuffle=is_train)","metadata":{"id":"1H9nw2k6po9a","execution":{"iopub.status.busy":"2021-08-07T12:21:45.089256Z","iopub.execute_input":"2021-08-07T12:21:45.089822Z","iopub.status.idle":"2021-08-07T12:21:45.097821Z","shell.execute_reply.started":"2021-08-07T12:21:45.089785Z","shell.execute_reply":"2021-08-07T12:21:45.097122Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"data_arrays = (hindi_list_indices, english_list_indices)\ndata_iter = load_array(data_arrays, batch_size)","metadata":{"id":"dYB9thb3zO51","execution":{"iopub.status.busy":"2021-08-07T12:21:45.100830Z","iopub.execute_input":"2021-08-07T12:21:45.101119Z","iopub.status.idle":"2021-08-07T12:21:45.107037Z","shell.execute_reply.started":"2021-08-07T12:21:45.101095Z","shell.execute_reply":"2021-08-07T12:21:45.106394Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### **Translate Hindi Sentence to English Sentence**","metadata":{}},{"cell_type":"code","source":"def translate_sentence(model, sentence, device, max_length=max_len):\n\n    sent_list=[]\n    for t in indic_tokenize.trivial_tokenize(sentence): \n      x = re.findall(\"[\\u0901-\\u0964A-Za-z.?!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\", t)\n      for elem in x:\n        if hindi_word_to_index.get(elem) is None:\n          continue\n        else:\n           sent_list.append(hindi_word_to_index[elem]) \n\n    sent_list.insert(0, hindi_word_to_index['SOS'])\n    sent_list.append(hindi_word_to_index['EOS'])\n\n    while(len(sent_list)<max_len):\n      sent_list.append(hindi_word_to_index['PAD'])\n\n\n    sent_tensor = torch.tensor(sent_list, dtype=torch.long).unsqueeze(1).to(device)\n\n    with torch.no_grad():\n        outputs_encoder, hidden = model.encoder(sent_tensor)\n\n    outputs = [2]  #SOS = 2\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden = model.decoder(previous_word, outputs_encoder, hidden)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        #EOS=3\n        if output.argmax(1).item() == 3:\n            break\n\n    translated_sentence = [english_index_to_word[idx] for idx in outputs]\n\n    return translated_sentence[1:]","metadata":{"id":"wvXjTt4XcXqv","execution":{"iopub.status.busy":"2021-08-07T12:21:45.109721Z","iopub.execute_input":"2021-08-07T12:21:45.110333Z","iopub.status.idle":"2021-08-07T12:21:45.120844Z","shell.execute_reply.started":"2021-08-07T12:21:45.110274Z","shell.execute_reply":"2021-08-07T12:21:45.119951Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### **Save Model**","metadata":{}},{"cell_type":"code","source":"def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n    torch.save(state, './checkpoint-NMT')\n    torch.save(model.state_dict(),'./checkpoint-NMT-SD')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T12:21:45.134875Z","iopub.execute_input":"2021-08-07T12:21:45.135221Z","iopub.status.idle":"2021-08-07T12:21:45.142287Z","shell.execute_reply.started":"2021-08-07T12:21:45.135189Z","shell.execute_reply":"2021-08-07T12:21:45.141632Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"### **Train Model**","metadata":{}},{"cell_type":"code","source":"import random\nimport sys\nepoch_loss = 0.0\nnum_epochs = 10\nbest_loss = sys.maxsize\nbest_epoch = -1\nsentence1 = \"मैं कहाँ रहते हैं आप जानते हो?\"\nstep=0\ni=0\nfor epoch in range(num_epochs):\n\n  print(\"Epoch -\",epoch+1)\n  model.eval()\n  print(max_len)\n  translated_sentence1 = translate_sentence(model, sentence1, device, max_length=max_len)\n\n  model.train(True)\n  for batch_idx, batch in enumerate(data_iter):\n    input, target = [x.to(device) for x in batch]\n\n    input = input.permute(1,0)\n    target = target.permute(1,0)\n\n\n    output = model(input, target)\n    output = output[1:].reshape(-1, output.shape[2])\n    target = target[1:].reshape(-1)\n\n    optimizer.zero_grad()\n\n    loss = criterion(output, target)\n\n    loss.backward()\n\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n    optimizer.step()\n    step += 1\n      \n    epoch_loss += loss.item()\n\n  if epoch_loss < best_loss:\n    best_loss = epoch_loss\n    best_epoch = epoch\n    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n\n  print(\"Epoch_Loss - {}\".format(loss.item()))\n  print()\n  \nprint(epoch_loss / len(data_iter))","metadata":{"id":"pTfYPg54bIL1","execution":{"iopub.status.busy":"2021-08-07T12:21:45.143869Z","iopub.execute_input":"2021-08-07T12:21:45.144248Z","iopub.status.idle":"2021-08-07T13:36:26.350321Z","shell.execute_reply.started":"2021-08-07T12:21:45.144213Z","shell.execute_reply":"2021-08-07T13:36:26.349180Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Epoch - 1\n35\nEpoch_Loss - 5.104295253753662\n\nEpoch - 2\n35\nEpoch_Loss - 4.499795913696289\n\nEpoch - 3\n35\nEpoch_Loss - 4.5629730224609375\n\nEpoch - 4\n35\nEpoch_Loss - 3.321319818496704\n\nEpoch - 5\n35\nEpoch_Loss - 4.023654460906982\n\nEpoch - 6\n35\nEpoch_Loss - 3.695626974105835\n\nEpoch - 7\n35\nEpoch_Loss - 3.640519380569458\n\nEpoch - 8\n35\nEpoch_Loss - 3.675459384918213\n\nEpoch - 9\n35\nEpoch_Loss - 3.4773542881011963\n\nEpoch - 10\n35\nEpoch_Loss - 3.014796257019043\n\n39.57087118097455\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Translate the Hindi Sentences from the Test Set**","metadata":{}},{"cell_type":"code","source":"outputs = []\n\nfor src,trg in zip(hindi_test_sentence_list, english_test_sentence_list):\n    prediction = translate_sentence(model, src, device)\n    prediction = prediction[:-1]  # remove <eos> token\n    x = ' '.join([e for e in prediction])\n    outputs.append(x)","metadata":{"id":"Z2huZfH1B6Gx","execution":{"iopub.status.busy":"2021-08-07T13:36:26.353693Z","iopub.execute_input":"2021-08-07T13:36:26.354007Z","iopub.status.idle":"2021-08-07T13:40:50.010232Z","shell.execute_reply.started":"2021-08-07T13:36:26.353956Z","shell.execute_reply":"2021-08-07T13:40:50.009291Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"### **Compute BLEU and METEOR score on Test Set**","metadata":{}},{"cell_type":"code","source":"!pip install -U nltk","metadata":{"id":"xwnGPqaREaa1","execution":{"iopub.status.busy":"2021-08-07T13:40:50.011730Z","iopub.execute_input":"2021-08-07T13:40:50.012211Z","iopub.status.idle":"2021-08-07T13:40:59.211680Z","shell.execute_reply.started":"2021-08-07T13:40:50.012171Z","shell.execute_reply":"2021-08-07T13:40:59.210817Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n\u001b[K     |████████████████████████████████| 1.5 MB 606 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.56.2)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.0.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from nltk) (2020.11.13)\nInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.6.2 which is incompatible.\u001b[0m\nSuccessfully installed nltk-3.6.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nimport sys\nnltk.download('wordnet')\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.meteor_score import single_meteor_score\n\nreferences = english_test_sentence_list\n\nhypotheses = outputs\n\ntotal_num = len(references)\ntotal_bleu_scores = 0\ntotal_meteor_scores = 0\nfor i in range(total_num):\n  total_bleu_scores+=sentence_bleu([references[i].split(\" \")], hypotheses[i].split(\" \"))\n  total_meteor_scores+=single_meteor_score(references[i], hypotheses[i])\n\nbleu_result = total_bleu_scores/total_num\nmeteor_result = total_meteor_scores/total_num\n\nprint(\"bleu score: \",bleu_result)\nprint(\"meteor score: \",meteor_result)","metadata":{"id":"C32flvd6BuBK","execution":{"iopub.status.busy":"2021-08-07T13:40:59.215254Z","iopub.execute_input":"2021-08-07T13:40:59.215555Z","iopub.status.idle":"2021-08-07T13:41:20.930466Z","shell.execute_reply.started":"2021-08-07T13:40:59.215526Z","shell.execute_reply":"2021-08-07T13:41:20.929590Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \nThe hypothesis contains 0 counts of 2-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \nThe hypothesis contains 0 counts of 3-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \nThe hypothesis contains 0 counts of 4-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"bleu score:  0.0011523031828677911\nmeteor score:  0.16078945059348346\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Load Hindi dataset**","metadata":{}},{"cell_type":"code","source":"testpath = '../input/hindi4/hindistatements4.csv'","metadata":{"id":"VMmj7YiMul8b","execution":{"iopub.status.busy":"2021-08-07T13:41:20.931755Z","iopub.execute_input":"2021-08-07T13:41:20.932264Z","iopub.status.idle":"2021-08-07T13:41:20.941164Z","shell.execute_reply.started":"2021-08-07T13:41:20.932223Z","shell.execute_reply":"2021-08-07T13:41:20.940367Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"import csv\n\nfinaldata=[]\n\nwith open(testpath, 'r') as f:\n    reader = csv.DictReader(f)\n    for line in reader:\n        finaldata.append(line['hindi'])","metadata":{"id":"VDa3RA3WukcC","execution":{"iopub.status.busy":"2021-08-07T13:41:20.942458Z","iopub.execute_input":"2021-08-07T13:41:20.942789Z","iopub.status.idle":"2021-08-07T13:41:21.009399Z","shell.execute_reply.started":"2021-08-07T13:41:20.942756Z","shell.execute_reply":"2021-08-07T13:41:21.008717Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"### **Store the corresponding Translated English Sentences**","metadata":{}},{"cell_type":"code","source":"with open('./answer.txt', 'w') as f:\n    for sent in finaldata:\n        prediction = translate_sentence(model, sent, device)\n        prediction = prediction[:-1]  # remove <eos> token\n        x = ' '.join([e for e in prediction])\n        f.write(x+'\\n')","metadata":{"id":"88dzuUTlHmd1","execution":{"iopub.status.busy":"2021-08-07T13:41:21.010609Z","iopub.execute_input":"2021-08-07T13:41:21.010926Z","iopub.status.idle":"2021-08-07T13:42:25.070369Z","shell.execute_reply.started":"2021-08-07T13:41:21.010899Z","shell.execute_reply":"2021-08-07T13:42:25.069583Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"JYiI_qjeHNfc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}